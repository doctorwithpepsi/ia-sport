{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGlQvxbgViSF"
      },
      "source": [
        "**Principaux ensembles de donnÃ©es sur le basket-ball**\n",
        "Les ensembles de donnÃ©es de basketball et les modÃ¨les de vision par ordinateur permettent de fournir des analyses en temps rÃ©el et des analyses d'aprÃ¨s-match des statistiques clÃ©s du basketball. La vision par ordinateur permet de collecter automatiquement des donnÃ©es issues de matchs universitaires (par exemple, NCAA) ou professionnels (par exemple, NBA), telles que le nombre de passes, le nombre de tirs, le temps de possession d'une Ã©quipe, le temps de possession d'un joueur, les passes dÃ©cisives, les dunks, et bien plus encore. DÃ©couvrez un tutoriel vidÃ©o illustrant l'utilisation de la vision par ordinateur pour suivre les joueurs avec YOLOv5 :\n",
        "https://www.youtube.com/watch?v=QCG8QMhga9k\n",
        "\n",
        "\n",
        "https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-make-or-miss-jumpshot-detection.ipynb#scrollTo=0hPkIcyNc0kB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zw-RlsjndH0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabaaf9b-d2e8-4f7a-b91d-1395e729beb1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDLlx9EqbWeH"
      },
      "source": [
        "https://universe.roboflow.com/roboflow-universe-projects/basketball-players-fy4c2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snNK_K8mR2te",
        "outputId": "3bcd37ba-418d-45c1-a9aa-e52add2b484c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.248-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.248-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.248 ultralytics-thop-2.0.18\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.11.12)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.61.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaz--1TOe6Al",
        "outputId": "547c9c35-8f9d-49a9-8b78-f2e69bd94f17",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"SfJv3Xnc96eIavDJMgXj\")\n",
        "project = rf.workspace(\"lpid-1yl6j\").project(\"basketball-player-detection-3-ycjdo-yehg8\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8RPm00UiBJ5",
        "outputId": "74c25bda-052e-40be-f4e5-2c66aad818d5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New https://pypi.org/project/ultralytics/8.3.206 available ğŸ˜ƒ Update with 'pip install -U ultralytics'\n",
            "Ultralytics 8.3.205 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/basketball-player-detection-3-1/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=10\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,012,798 parameters, 3,012,782 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 77.5Â±16.5 MB/s, size: 256.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/basketball-player-detection-3-1/train/labels.cache... 464 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 464/464 786.6Kit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 54.2Â±17.1 MB/s, size: 258.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/basketball-player-detection-3-1/valid/labels.cache... 96 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 96/96 59.1Kit/s 0.0s\n",
            "Plotting labels to /content/runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train2\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/100      2.76G      1.716      3.527      1.286        594        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.1it/s 13.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.7it/s 1.8s\n",
            "                   all         96       1953     0.0231      0.399     0.0881     0.0424\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/100      2.78G      1.469      1.758      1.099        614        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.1it/s 1.4s\n",
            "                   all         96       1953     0.0311      0.329     0.0891     0.0527\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      3/100      2.78G      1.341      1.411      1.073        574        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953      0.897      0.149      0.169      0.105\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      4/100      2.79G      1.288      1.219      1.045        476        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.796      0.206      0.224      0.131\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      5/100      2.79G      1.241      1.125      1.026        626        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.4it/s 1.3s\n",
            "                   all         96       1953      0.811      0.208      0.241       0.15\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      6/100      2.79G      1.223      1.046      1.015        696        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.3s\n",
            "                   all         96       1953      0.789       0.27        0.3      0.139\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      7/100      2.81G      1.146     0.9633     0.9992        727        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.833      0.315      0.364      0.199\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      8/100      2.82G      1.163     0.9404          1        563        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.812      0.334      0.371      0.205\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      9/100      2.82G      1.102     0.9017     0.9821        581        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.864      0.356      0.417      0.224\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     10/100      2.82G      1.091     0.8782     0.9774        598        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.1it/s 1.4s\n",
            "                   all         96       1953      0.834      0.385      0.453      0.259\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     11/100      2.82G      1.066     0.8451     0.9735        562        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.1it/s 1.4s\n",
            "                   all         96       1953      0.806      0.433      0.491      0.273\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     12/100      2.84G      1.042     0.8113      0.966        670        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.1it/s 1.4s\n",
            "                   all         96       1953      0.831      0.442      0.505      0.276\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     13/100      2.86G      1.035      0.799     0.9608        575        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.8it/s 1.6s\n",
            "                   all         96       1953      0.855      0.455       0.54      0.293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     14/100      2.88G      1.058     0.7849     0.9656        546        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.8it/s 1.7s\n",
            "                   all         96       1953      0.801      0.448      0.525      0.296\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     15/100      2.88G      1.065     0.7763     0.9644        639        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.3it/s 2.3s\n",
            "                   all         96       1953      0.854      0.496      0.559      0.317\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     16/100      2.88G      1.015     0.7635     0.9549        580        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s\n",
            "                   all         96       1953      0.872      0.472      0.575      0.324\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     17/100      2.88G     0.9749     0.7303     0.9437        512        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.6it/s 1.1s\n",
            "                   all         96       1953      0.729      0.519      0.573      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     18/100      2.88G     0.9842     0.7289     0.9435        602        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.3s\n",
            "                   all         96       1953      0.655      0.512      0.541      0.293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     19/100      2.88G     0.9622     0.7192     0.9387        546        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.609      0.461      0.552       0.31\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     20/100      2.88G     0.9731      0.715     0.9368        516        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.847      0.512      0.589       0.32\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     21/100      2.88G     0.9709     0.7058     0.9421        625        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.4it/s 1.3s\n",
            "                   all         96       1953      0.765      0.495      0.572      0.326\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     22/100      2.88G     0.9572     0.6838     0.9341        642        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.6it/s 1.1s\n",
            "                   all         96       1953      0.754      0.531      0.617      0.353\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     23/100      2.88G     0.9354     0.6768     0.9294        539        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.3s\n",
            "                   all         96       1953      0.792      0.509      0.603       0.34\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     24/100      2.88G     0.9445     0.6692     0.9227        620        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.4it/s 1.2s\n",
            "                   all         96       1953      0.774      0.512      0.584      0.316\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     25/100      2.88G     0.9345      0.659     0.9264        628        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.8it/s 1.1s\n",
            "                   all         96       1953      0.871      0.483      0.616      0.342\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     26/100      2.88G     0.9298     0.6514     0.9265        683        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953      0.762      0.513       0.58      0.329\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     27/100      2.88G      0.913     0.6498     0.9194        615        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.713      0.525       0.61      0.353\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     28/100      2.88G     0.9294     0.6492     0.9294        662        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.737       0.54      0.608      0.356\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     29/100      2.88G     0.9168      0.631     0.9184        689        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.0it/s 1.5s\n",
            "                   all         96       1953      0.803      0.549      0.628      0.373\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     30/100      2.88G     0.9072     0.6239     0.9228        655        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.3it/s 2.2s\n",
            "                   all         96       1953      0.748      0.551      0.625      0.373\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     31/100      2.88G     0.9053       0.62     0.9216        505        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.4it/s 2.2s\n",
            "                   all         96       1953      0.833      0.539      0.614       0.36\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     32/100      2.88G     0.9066     0.6131     0.9158        510        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.4it/s 8.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.0it/s 1.5s\n",
            "                   all         96       1953      0.783      0.609      0.648      0.385\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     33/100      2.89G     0.8953     0.6099     0.9154        526        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.8it/s 1.1s\n",
            "                   all         96       1953      0.789      0.554      0.645      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     34/100      2.89G     0.8982     0.6058     0.9178        657        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953      0.743      0.567      0.647      0.384\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     35/100      2.89G     0.8934     0.5987     0.9145        584        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.6it/s 11.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953      0.687      0.623      0.671      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     36/100      2.89G     0.8918     0.5945     0.9171        517        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953      0.599      0.624       0.68      0.398\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     37/100      2.89G      0.893     0.5878     0.9132        511        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.8it/s 1.1s\n",
            "                   all         96       1953      0.651      0.632      0.666      0.397\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     38/100      2.89G     0.8961     0.5832     0.9151        676        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.706       0.64      0.692      0.427\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     39/100      2.89G     0.8788     0.5792     0.9091        631        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.8it/s 1.1s\n",
            "                   all         96       1953      0.767        0.6      0.665      0.408\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     40/100      2.89G      0.871     0.5757     0.9106        659        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.789       0.61      0.684      0.423\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     41/100      2.89G     0.8709     0.5691     0.9091        769        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.8it/s 1.1s\n",
            "                   all         96       1953      0.644      0.662      0.679      0.416\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     42/100      2.89G      0.866      0.566     0.9019        518        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.683       0.66      0.683      0.423\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     43/100      2.89G     0.8596      0.556     0.9051        591        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.664      0.705      0.695      0.426\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     44/100      2.89G      0.872     0.5646     0.9063        589        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.696      0.678      0.705      0.438\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     45/100      2.89G      0.859     0.5574     0.9044        533        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.795      0.598      0.701      0.446\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     46/100      2.89G     0.8579     0.5543     0.9038        530        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.6it/s 1.2s\n",
            "                   all         96       1953      0.764      0.653      0.723      0.449\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     47/100      2.89G     0.8314     0.5447     0.8974        612        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.4it/s 1.2s\n",
            "                   all         96       1953      0.822      0.651      0.742      0.467\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     48/100      2.89G     0.8372     0.5445     0.8952        565        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.3s\n",
            "                   all         96       1953      0.778      0.643      0.708      0.436\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     49/100      2.89G     0.8457     0.5424     0.9002        775        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.4it/s 1.3s\n",
            "                   all         96       1953      0.797      0.686      0.717      0.449\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     50/100      2.89G     0.8368     0.5415     0.9006        622        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953      0.797      0.652      0.734      0.462\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     51/100      2.89G     0.8336     0.5327     0.8945        632        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953       0.83      0.617      0.707      0.446\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     52/100      2.89G     0.8279     0.5302     0.8951        671        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.1it/s 1.5s\n",
            "                   all         96       1953      0.682      0.655      0.685       0.43\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     53/100      2.89G     0.8288      0.533     0.8994        436        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.0it/s 1.5s\n",
            "                   all         96       1953      0.677      0.688      0.707      0.449\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     54/100      2.89G     0.8224     0.5279     0.8933        623        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.7it/s 1.7s\n",
            "                   all         96       1953      0.658      0.673      0.707      0.442\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     55/100      2.89G     0.8319     0.5303     0.8953        546        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.4it/s 2.1s\n",
            "                   all         96       1953      0.703      0.682      0.712      0.456\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     56/100      2.89G     0.8103     0.5176     0.8921        529        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.2it/s 8.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s\n",
            "                   all         96       1953       0.74      0.675      0.716      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     57/100      2.89G     0.8089     0.5138     0.8911        583        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.2it/s 9.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.7it/s 1.7s\n",
            "                   all         96       1953      0.744       0.68      0.718      0.455\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     58/100      2.89G     0.8142     0.5151     0.8904        600        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.2it/s 9.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.0it/s 1.5s\n",
            "                   all         96       1953       0.71      0.702      0.718      0.451\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     59/100      2.89G     0.8208     0.5231     0.8993        593        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.4it/s 1.3s\n",
            "                   all         96       1953      0.814      0.671      0.717      0.456\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     60/100      2.89G     0.8028     0.5092     0.8912        598        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.846      0.666      0.733      0.461\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     61/100      2.89G     0.8184     0.5214     0.8939        708        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.829      0.643      0.734      0.468\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     62/100      2.89G     0.8102     0.5147     0.8911        596        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.3s\n",
            "                   all         96       1953      0.715      0.686      0.723       0.46\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     63/100      2.89G     0.8128     0.5136     0.8922        544        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.0it/s 1.5s\n",
            "                   all         96       1953       0.74      0.701      0.742      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     64/100      2.89G     0.8066     0.5027     0.8863        579        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.4it/s 1.2s\n",
            "                   all         96       1953        0.7      0.675      0.716      0.472\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     65/100      2.89G     0.7894     0.4979     0.8883        637        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.4it/s 1.2s\n",
            "                   all         96       1953       0.74      0.706      0.721      0.456\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     66/100      2.89G      0.781     0.4897     0.8856        770        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.1it/s 1.4s\n",
            "                   all         96       1953      0.787      0.678      0.734      0.472\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     67/100      2.89G     0.7954     0.5009      0.889        543        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.784      0.711      0.752      0.474\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     68/100      2.89G     0.7816      0.491     0.8843        537        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.738      0.688      0.723      0.455\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     69/100      2.89G     0.7867     0.4973     0.8882        565        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.742       0.68      0.721      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     70/100      2.89G     0.7793     0.4934     0.8828        633        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.1it/s 1.4s\n",
            "                   all         96       1953      0.759      0.667      0.709      0.465\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     71/100      2.89G      0.788     0.4942     0.8854        581        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.0it/s 1.5s\n",
            "                   all         96       1953      0.772      0.669      0.729      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     72/100      2.89G     0.7827     0.4877     0.8845        617        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953       0.78      0.667       0.73      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     73/100      2.89G     0.7697     0.4803     0.8825        559        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953       0.74      0.679      0.746      0.489\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     74/100      2.89G     0.7859     0.4917     0.8861        606        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.7it/s 1.7s\n",
            "                   all         96       1953      0.741      0.704      0.739      0.482\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     75/100      2.89G     0.7718     0.4834     0.8803        751        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.8it/s 1.7s\n",
            "                   all         96       1953      0.848      0.651       0.73      0.478\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     76/100      2.89G     0.7622     0.4785     0.8773        675        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.6it/s 1.9s\n",
            "                   all         96       1953      0.896      0.626      0.728      0.478\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     77/100      2.89G     0.7724     0.4784     0.8826        468        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 1.9s\n",
            "                   all         96       1953      0.827      0.676      0.744       0.48\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     78/100      2.89G     0.7743     0.4807     0.8815        660        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s\n",
            "                   all         96       1953      0.772      0.693      0.738      0.478\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     79/100      2.89G     0.7587     0.4708     0.8821        604        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.7it/s 1.8s\n",
            "                   all         96       1953      0.756      0.704      0.735      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     80/100      2.89G     0.7572     0.4738     0.8768        655        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.2it/s 9.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953      0.749      0.669      0.716      0.477\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     81/100      2.89G     0.7649     0.4735     0.8808        576        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.0it/s 9.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.821      0.653      0.725      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     82/100      2.89G     0.7697     0.4765     0.8799        621        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.7it/s 1.7s\n",
            "                   all         96       1953      0.831      0.674      0.734      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     83/100      2.89G     0.7636     0.4719     0.8806        647        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.3s\n",
            "                   all         96       1953      0.786      0.694      0.753      0.498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     84/100      2.89G     0.7502     0.4649     0.8772        554        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.748      0.697      0.733      0.493\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     85/100      2.89G     0.7527     0.4652     0.8805        636        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.773      0.669      0.715      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     86/100      2.89G     0.7459     0.4613      0.876        607        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.6it/s 1.1s\n",
            "                   all         96       1953      0.784      0.674      0.739      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     87/100      2.89G     0.7513     0.4667     0.8759        640        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.4it/s 1.3s\n",
            "                   all         96       1953      0.784      0.653      0.738      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     88/100      2.89G     0.7477      0.463     0.8783        496        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.7it/s 1.1s\n",
            "                   all         96       1953      0.772      0.685      0.729      0.479\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     89/100      2.89G     0.7562     0.4705     0.8793        736        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.7it/s 10.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.6it/s 1.2s\n",
            "                   all         96       1953      0.813      0.652      0.729      0.479\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     90/100      2.89G     0.7387     0.4575     0.8732        624        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.8it/s 10.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.856      0.663      0.743      0.491\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     91/100      2.89G      0.748     0.4772     0.8632        316        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.3it/s 12.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953      0.725      0.669      0.708      0.458\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     92/100      2.89G     0.7054     0.4357     0.8521        320        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953      0.678      0.658      0.691      0.439\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     93/100      2.89G     0.7165     0.4417     0.8544        310        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.742      0.658        0.7      0.451\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     94/100      2.89G     0.6967     0.4281     0.8503        328        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.2it/s 1.4s\n",
            "                   all         96       1953      0.755      0.629      0.686      0.445\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     95/100      2.89G      0.687     0.4208       0.85        323        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 9.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.1it/s 1.4s\n",
            "                   all         96       1953      0.793      0.618      0.706      0.459\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     96/100      2.89G     0.6902     0.4209     0.8461        303        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.5it/s 1.2s\n",
            "                   all         96       1953       0.73      0.672      0.704      0.454\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     97/100      2.89G     0.6807     0.4191     0.8489        314        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.1it/s 9.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.3it/s 1.3s\n",
            "                   all         96       1953      0.743      0.655      0.706      0.456\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     98/100      2.89G     0.6784     0.4134     0.8451        324        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 2.9it/s 10.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 2.0it/s 1.5s\n",
            "                   all         96       1953      0.766      0.646      0.709       0.46\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     99/100      2.89G     0.6833     0.4185     0.8489        306        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.4it/s 8.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.5it/s 2.0s\n",
            "                   all         96       1953       0.77      0.641      0.711      0.462\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    100/100      2.89G      0.673     0.4122     0.8498        312        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 29/29 3.3it/s 8.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.2it/s 2.5s\n",
            "                   all         96       1953       0.81      0.637      0.713      0.464\n",
            "\n",
            "100 epochs completed in 0.349 hours.\n",
            "Optimizer stripped from /content/runs/detect/train2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/train2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/train2/weights/best.pt...\n",
            "Ultralytics 8.3.205 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,598 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 0.9it/s 3.3s\n",
            "                   all         96       1953      0.785      0.692       0.75      0.497\n",
            "                  ball         88         88      0.779      0.601      0.641      0.356\n",
            "        ball-in-basket          9          9      0.679      0.444      0.587      0.399\n",
            "                number         96        522      0.813      0.674      0.751      0.316\n",
            "                player         96        908       0.87      0.954      0.966      0.745\n",
            "  player-in-possession         17         17      0.716      0.471      0.509      0.389\n",
            "      player-jump-shot         17         17      0.573      0.475      0.604      0.423\n",
            "     player-shot-block          9          9      0.735      0.667      0.717      0.473\n",
            "               referee         96        287      0.968      0.955      0.989      0.761\n",
            "                   rim         96         96       0.93       0.99      0.984      0.611\n",
            "Speed: 0.2ms preprocess, 2.2ms inference, 0.0ms loss, 7.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train2\u001b[0m\n",
            "Ultralytics 8.3.205 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,007,598 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3843.5Â±888.8 MB/s, size: 252.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/basketball-player-detection-3-1/valid/labels.cache... 96 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 96/96 173.3Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 1.1it/s 5.3s\n",
            "                   all         96       1953      0.786      0.692      0.751      0.498\n",
            "                  ball         88         88      0.779      0.601      0.641      0.356\n",
            "        ball-in-basket          9          9      0.671      0.444      0.606      0.394\n",
            "                number         96        522      0.812      0.672      0.743      0.318\n",
            "                player         96        908      0.871      0.954      0.965      0.745\n",
            "  player-in-possession         17         17      0.726      0.471      0.505      0.389\n",
            "      player-jump-shot         17         17      0.574      0.476      0.604      0.428\n",
            "     player-shot-block          9          9      0.737      0.667      0.717      0.489\n",
            "               referee         96        287      0.968      0.953      0.989       0.76\n",
            "                   rim         96         96      0.939       0.99      0.986      0.607\n",
            "Speed: 3.6ms preprocess, 7.4ms inference, 0.0ms loss, 4.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val\u001b[0m\n",
            "Ultralytics 8.3.205 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CPU (Intel Xeon CPU @ 2.00GHz)\n",
            "ğŸ’¡ ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/runs/detect/train2/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 14, 8400) (5.9 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim>=0.1.67', 'onnxruntime-gpu'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 6.5s\n",
            "WARNING âš ï¸ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1rc1 opset 22...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.70...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 7.8s, saved as '/content/runs/detect/train2/weights/best.onnx' (11.7 MB)\n",
            "\n",
            "Export complete (8.1s)\n",
            "Results saved to \u001b[1m/content/runs/detect/train2/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=/content/runs/detect/train2/weights/best.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=/content/runs/detect/train2/weights/best.onnx imgsz=640 data=/content/basketball-player-detection-3-1/data.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8n.yaml\")\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "\n",
        "model.train(data=f\"{dataset.location}/data.yaml\", epochs=100)\n",
        "metrics = model.val()\n",
        "\n",
        "path = model.export(format=\"onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Charger ton modÃ¨le\n",
        "model = YOLO(\"/content/runs/detect/train2/weights/best.pt\")\n",
        "\n",
        "# Chemin de la vidÃ©o d'entrÃ©e\n",
        "video_path = \"/content/basketball/dunk/v_BasketballDunk_g01_c01.avi\"\n",
        "\n",
        "# Faire la prÃ©diction sur la vidÃ©o\n",
        "results = model.predict(\n",
        "    source=video_path,   # chemin vidÃ©o\n",
        "    conf=0.25,           # seuil de confiance\n",
        "    save=True,           # sauvegarde la vidÃ©o avec bounding boxes\n",
        "    show=False           # show=True ne marche pas dans Colab\n",
        "\n",
        ")\n",
        "\n",
        "# VÃ©rifier oÃ¹ la vidÃ©o encadrÃ©e est sauvegardÃ©e\n",
        "output_dir = \"runs/detect/predict\"\n",
        "video_name = os.path.basename(video_path)  # garde le mÃªme nom que la vidÃ©o d'entrÃ©e\n",
        "output_video_path = os.path.join(output_dir, video_name)\n",
        "\n",
        "# Renommer et dÃ©placer pour plus de clartÃ©\n",
        "final_video_path = \"/content/result.mp4\"\n",
        "if os.path.exists(output_video_path):\n",
        "    shutil.copy(output_video_path, final_video_path)\n",
        "    files.download(final_video_path)\n",
        "else:\n",
        "    print(\"âš ï¸ VidÃ©o de sortie non trouvÃ©e\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-WxrTdg2UH-",
        "outputId": "7619cce4-39e1-4c14-c37e-3accc0a11e68",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING âš ï¸ \n",
            "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 16 players, 1 player-in-possession, 1 player-jump-shot, 1 rim, 9.3ms\n",
            "video 1/1 (frame 2/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 16 players, 1 player-jump-shot, 1 rim, 6.2ms\n",
            "video 1/1 (frame 3/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 20 players, 1 player-in-possession, 1 player-jump-shot, 1 player-shot-block, 1 rim, 5.9ms\n",
            "video 1/1 (frame 4/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 numbers, 16 players, 2 rims, 5.9ms\n",
            "video 1/1 (frame 5/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 15 players, 1 rim, 5.9ms\n",
            "video 1/1 (frame 6/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 number, 18 players, 1 referee, 2 rims, 6.1ms\n",
            "video 1/1 (frame 7/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 number, 17 players, 1 player-in-possession, 1 rim, 7.2ms\n",
            "video 1/1 (frame 8/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 ball, 2 numbers, 20 players, 1 player-in-possession, 1 player-shot-block, 1 rim, 6.0ms\n",
            "video 1/1 (frame 9/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 balls, 3 numbers, 13 players, 1 player-shot-block, 6.1ms\n",
            "video 1/1 (frame 10/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 numbers, 14 players, 1 rim, 7.1ms\n",
            "video 1/1 (frame 11/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 numbers, 15 players, 1 player-in-possession, 2 rims, 7.2ms\n",
            "video 1/1 (frame 12/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 balls, 2 numbers, 13 players, 2 player-in-possessions, 1 referee, 1 rim, 6.2ms\n",
            "video 1/1 (frame 13/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 ball, 1 number, 11 players, 3 player-in-possessions, 1 referee, 1 rim, 5.9ms\n",
            "video 1/1 (frame 14/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 balls, 1 number, 12 players, 4 player-in-possessions, 1 referee, 1 rim, 5.9ms\n",
            "video 1/1 (frame 15/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 balls, 3 numbers, 11 players, 6 player-in-possessions, 1 referee, 1 rim, 5.9ms\n",
            "video 1/1 (frame 16/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 balls, 3 numbers, 12 players, 3 player-in-possessions, 1 referee, 1 rim, 7.5ms\n",
            "video 1/1 (frame 17/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 balls, 3 numbers, 13 players, 2 player-in-possessions, 1 referee, 1 rim, 5.9ms\n",
            "video 1/1 (frame 18/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 balls, 2 numbers, 14 players, 2 player-in-possessions, 1 rim, 5.9ms\n",
            "video 1/1 (frame 19/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 ball, 3 numbers, 13 players, 3 player-in-possessions, 1 rim, 5.9ms\n",
            "video 1/1 (frame 20/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 4 numbers, 14 players, 1 player-in-possession, 1 rim, 5.9ms\n",
            "video 1/1 (frame 21/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 4 numbers, 14 players, 3 player-in-possessions, 1 rim, 6.4ms\n",
            "video 1/1 (frame 22/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 5 numbers, 14 players, 3 player-in-possessions, 1 rim, 5.9ms\n",
            "video 1/1 (frame 23/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 4 numbers, 14 players, 1 player-in-possession, 1 rim, 5.9ms\n",
            "video 1/1 (frame 24/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 4 numbers, 13 players, 1 player-in-possession, 1 rim, 5.8ms\n",
            "video 1/1 (frame 25/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 4 numbers, 15 players, 1 rim, 5.9ms\n",
            "video 1/1 (frame 26/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 4 numbers, 11 players, 1 rim, 7.5ms\n",
            "video 1/1 (frame 27/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 14 players, 1 rim, 5.9ms\n",
            "video 1/1 (frame 28/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 ball, 3 numbers, 14 players, 1 player-in-possession, 1 rim, 5.9ms\n",
            "video 1/1 (frame 29/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 ball, 2 numbers, 13 players, 1 player-in-possession, 1 rim, 5.9ms\n",
            "video 1/1 (frame 30/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 ball, 3 numbers, 13 players, 1 player-in-possession, 1 rim, 6.0ms\n",
            "video 1/1 (frame 31/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 12 players, 2 player-in-possessions, 1 referee, 1 rim, 6.2ms\n",
            "video 1/1 (frame 32/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 numbers, 14 players, 1 player-in-possession, 1 referee, 1 rim, 5.9ms\n",
            "video 1/1 (frame 33/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 14 players, 2 player-in-possessions, 1 referee, 1 rim, 5.9ms\n",
            "video 1/1 (frame 34/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 13 players, 1 player-in-possession, 2 referees, 1 rim, 5.9ms\n",
            "video 1/1 (frame 35/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 numbers, 14 players, 1 referee, 1 rim, 6.4ms\n",
            "video 1/1 (frame 36/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 numbers, 13 players, 1 player-in-possession, 1 referee, 1 rim, 5.9ms\n",
            "video 1/1 (frame 37/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 number, 9 players, 2 referees, 1 rim, 5.9ms\n",
            "video 1/1 (frame 38/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 12 players, 1 referee, 1 rim, 6.0ms\n",
            "video 1/1 (frame 39/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 number, 13 players, 1 player-in-possession, 2 referees, 1 rim, 5.9ms\n",
            "video 1/1 (frame 40/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 13 players, 2 referees, 1 rim, 5.9ms\n",
            "video 1/1 (frame 41/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 ball-in-basket, 1 number, 13 players, 2 referees, 2 rims, 5.8ms\n",
            "video 1/1 (frame 42/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 number, 11 players, 1 referee, 2 rims, 5.6ms\n",
            "video 1/1 (frame 43/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 11 players, 1 player-jump-shot, 1 player-shot-block, 9.2ms\n",
            "video 1/1 (frame 44/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 10 players, 1 player-shot-block, 5.6ms\n",
            "video 1/1 (frame 45/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 10 players, 1 player-shot-block, 1 rim, 5.6ms\n",
            "video 1/1 (frame 46/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 ball, 16 players, 1 player-jump-shot, 2 referees, 5.6ms\n",
            "video 1/1 (frame 47/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 ball, 2 numbers, 16 players, 1 player-shot-block, 2 referees, 1 rim, 5.6ms\n",
            "video 1/1 (frame 48/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 11 players, 1 player-in-possession, 1 player-shot-block, 2 referees, 5.7ms\n",
            "video 1/1 (frame 49/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 10 players, 1 player-in-possession, 2 referees, 5.8ms\n",
            "video 1/1 (frame 50/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 10 players, 2 player-in-possessions, 1 referee, 9.7ms\n",
            "video 1/1 (frame 51/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 numbers, 13 players, 1 referee, 11.2ms\n",
            "video 1/1 (frame 52/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 13 players, 2 referees, 1 rim, 5.9ms\n",
            "video 1/1 (frame 53/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 number, 12 players, 1 referee, 1 rim, 5.9ms\n",
            "video 1/1 (frame 54/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 15 players, 2 referees, 1 rim, 5.8ms\n",
            "video 1/1 (frame 55/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 15 players, 1 referee, 2 rims, 5.6ms\n",
            "video 1/1 (frame 56/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 number, 15 players, 2 referees, 1 rim, 5.6ms\n",
            "video 1/1 (frame 57/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 1 number, 13 players, 3 referees, 1 rim, 5.6ms\n",
            "video 1/1 (frame 58/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 2 numbers, 12 players, 2 referees, 1 rim, 5.9ms\n",
            "video 1/1 (frame 59/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 numbers, 12 players, 2 referees, 1 rim, 7.3ms\n",
            "video 1/1 (frame 60/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 3 numbers, 11 players, 4 referees, 1 rim, 7.4ms\n",
            "video 1/1 (frame 61/61) /content/basketball/dunk/v_BasketballDunk_g01_c01.avi: 480x640 4 numbers, 14 players, 3 referees, 1 rim, 5.9ms\n",
            "Speed: 2.2ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1m/content/runs/detect/predict2\u001b[0m\n",
            "âš ï¸ VidÃ©o de sortie non trouvÃ©e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/shorts/HljiRrgGAsI - video basket court"
      ],
      "metadata": {
        "id": "ibfgNREJZdxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Chemin du fichier zip\n",
        "zip_path = \"dataset.zip\"\n",
        "\n",
        "# Dossier de destination (sera crÃ©Ã© s'il n'existe pas)\n",
        "extract_dir = \"/content/\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Extraction\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"âœ… Fichiers extraits dans : {extract_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqbtuVomZsfu",
        "outputId": "a1e2d73d-b994-4a35-835e-324954ab0e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fichiers extraits dans : /content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "from collections import Counter, deque\n",
        "\n",
        "# === 1 Charger le modÃ¨le YOLOv8 ===\n",
        "model = YOLO(\"best.pt\")\n",
        "\n",
        "# === 2 Classes ===\n",
        "keep_classes = [\n",
        "    \"ball\", \"ball-in-basket\", \"player-jump-shot\",\n",
        "    \"player-layup-dunk\", \"player-shot-block\",\n",
        "    \"player-in-possession\", \"rim\"\n",
        "]\n",
        "\n",
        "# === 3 Charger la vidÃ©o ===\n",
        "input_video = \"/content/vid3.mp4\"\n",
        "output_video = \"output_shoots_only.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(input_video)\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
        "w, h = int(cap.get(3)), int(cap.get(4))\n",
        "out = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
        "\n",
        "print(f\"\\n VidÃ©o chargÃ©e : {w}x{h} Ã  {fps:.2f} FPS\")\n",
        "\n",
        "# === 4 FenÃªtre dâ€™analyse (50 frames) ===\n",
        "window_size = 50\n",
        "recent_actions = deque(maxlen=window_size)\n",
        "frame_id = 0\n",
        "current_dominant = \"None\"\n",
        "\n",
        "# === 5 Liste des classes de tir ===\n",
        "shoot_classes = [\"player-jump-shot\", \"player-layup-dunk\", \"player-shot-block\"]\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_id += 1\n",
        "\n",
        "    # DÃ©tection YOLO\n",
        "    results = model.predict(frame, conf=0.25, verbose=False)\n",
        "    boxes = results[0].boxes\n",
        "    detected = [model.names[int(b.cls)] for b in boxes if model.names[int(b.cls)] in keep_classes]\n",
        "\n",
        "    # Identifier lâ€™action de la frame\n",
        "    if \"player-jump-shot\" in detected:\n",
        "        action = \"Jump Shot\"\n",
        "    elif \"player-layup-dunk\" in detected:\n",
        "        action = \"Layup/Dunk\"\n",
        "    elif \"player-shot-block\" in detected:\n",
        "        action = \"Shot Block\"\n",
        "    else:\n",
        "        action = \"None\"\n",
        "\n",
        "    recent_actions.append(action)\n",
        "\n",
        "    # === Calcul de lâ€™action dominante sur la fenÃªtre ===\n",
        "    if len(recent_actions) == window_size:\n",
        "        count = Counter(recent_actions)\n",
        "        total = sum(count.values())\n",
        "\n",
        "        # Trouver la classe la plus frÃ©quente (en %)\n",
        "        dominant_action, freq = max(count.items(), key=lambda x: x[1])\n",
        "        percent = (freq / total) * 100\n",
        "\n",
        "        if percent >= 30 and dominant_action != \"None\":\n",
        "            current_dominant = dominant_action\n",
        "        else:\n",
        "            current_dominant = \"None\"\n",
        "\n",
        "    # === Afficher uniquement les boxes des actions de tir ===\n",
        "    for b in boxes:\n",
        "        cls = model.names[int(b.cls)]\n",
        "        if cls not in shoot_classes:\n",
        "            continue  # on ignore les autres classes\n",
        "\n",
        "        x1, y1, x2, y2 = map(int, b.xyxy[0])\n",
        "        conf = float(b.conf)\n",
        "        color = (0, 255, 255) if cls == \"player-jump-shot\" else \\\n",
        "                (255, 0, 0) if cls == \"player-layup-dunk\" else \\\n",
        "                (0, 0, 255)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(frame, f\"{cls} {conf:.2f}\", (x1, y1 - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "    # === Afficher lâ€™action dominante sur la vidÃ©o ===\n",
        "    if current_dominant != \"None\":\n",
        "        cv2.putText(frame, f\"Dominant Action: {current_dominant}\",\n",
        "                    (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "    # Debug console\n",
        "    if frame_id % 30 == 0:\n",
        "        print(f\"[INFO] Frame {frame_id}: Action dominante = {current_dominant}\")\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(f\"\\n VidÃ©o sauvegardÃ©e : {output_video}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igFJ7j7-wQ94",
        "outputId": "17af8492-2a8d-4f6d-9835-aa7d472c417e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\n",
            "ğŸ¥ VidÃ©o chargÃ©e : 1280x720 Ã  23.98 FPS\n",
            "[INFO] Frame 30: Action dominante = None\n",
            "[INFO] Frame 60: Action dominante = Shot Block\n",
            "[INFO] Frame 90: Action dominante = Shot Block\n",
            "[INFO] Frame 120: Action dominante = None\n",
            "\n",
            "âœ… VidÃ©o sauvegardÃ©e : output_shoots_only.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C3D\n"
      ],
      "metadata": {
        "id": "JGtGVsBc5ltC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!pip install roboflow\n",
        "!pip install torch torchvision opencv-python numpy ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zTVLgBBF5vMr",
        "outputId": "117acf02-b27b-415a-a37c-d46140145dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.241-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.241-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.241 ultralytics-thop-2.0.18\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.11.12)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.61.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.241)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "class C3D(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(C3D, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
        "\n",
        "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        # Adaptive pooling to handle any input size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 4, 4))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc6 = nn.Linear(8192, 4096)\n",
        "        self.fc7 = nn.Linear(4096, 4096)\n",
        "        self.fc8 = nn.Linear(4096, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional feature extraction\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.relu(self.conv3a(x))\n",
        "        x = self.relu(self.conv3b(x))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.relu(self.conv4a(x))\n",
        "        x = self.relu(self.conv4b(x))\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        x = self.relu(self.conv5a(x))\n",
        "        x = self.relu(self.conv5b(x))\n",
        "        x = self.pool5(x)\n",
        "\n",
        "        # Adaptive pooling ensures consistent output size\n",
        "        x = self.adaptive_pool(x)\n",
        "\n",
        "        # Flatten: batch_size x (512 * 1 * 4 * 4) = batch_size x 8192\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.relu(self.fc6(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc7(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc8(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "k8PFu92z6Eea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video_clip(video_path, num_frames=16, size=(112, 112)):\n",
        "    \"\"\"\n",
        "    Load video and extract frames\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Sample frames uniformly\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            # Resize and normalize\n",
        "            frame = cv2.resize(frame, size)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Convert to tensor: (C, D, H, W)\n",
        "    if len(frames) == num_frames:\n",
        "        clip = np.array(frames).transpose(3, 0, 1, 2)  # (C, D, H, W)\n",
        "        clip = clip / 255.0  # Normalize to [0, 1]\n",
        "        return torch.FloatTensor(clip)\n",
        "\n",
        "    return None\n",
        "\n",
        "class BasketballActionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, num_frames=16, size=(112, 112)):\n",
        "        \"\"\"\n",
        "        root_dir structure:\n",
        "        - dunk/\n",
        "        - jump-shoot/\n",
        "        - lay-up/\n",
        "        \"\"\"\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.num_frames = num_frames\n",
        "        self.size = size\n",
        "\n",
        "        # Class mapping\n",
        "        self.classes = ['dunk', 'jump-shoot', 'lay-up']\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        # Collect all videos\n",
        "        self.samples = []\n",
        "        for class_name in self.classes:\n",
        "            class_dir = self.root_dir / class_name\n",
        "            if class_dir.exists():\n",
        "                videos = list(class_dir.glob('*.mp4')) + list(class_dir.glob('*.avi'))\n",
        "                for video_path in videos:\n",
        "                    self.samples.append((str(video_path), self.class_to_idx[class_name]))\n",
        "\n",
        "        print(f\"Found {len(self.samples)} videos\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, label = self.samples[idx]\n",
        "        clip = load_video_clip(video_path, self.num_frames, self.size)\n",
        "\n",
        "        if clip is None:\n",
        "            # Return a dummy clip if loading fails\n",
        "            clip = torch.zeros((3, self.num_frames, *self.size))\n",
        "\n",
        "        return clip, label\n",
        "\n",
        "\n",
        "def train_c3d(data_dir, epochs=50, batch_size=4, lr=0.001):\n",
        "  # function to train c3d model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Prepare dataset\n",
        "    dataset = BasketballActionDataset(data_dir)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize model\n",
        "    model = C3D(num_classes=3).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for clips, labels in train_loader:\n",
        "            clips, labels = clips.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(clips)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for clips, labels in val_loader:\n",
        "                clips, labels = clips.to(device), labels.to(device)\n",
        "                outputs = model(clips)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss/len(val_loader):.4f}, Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'c3d_basketball_best.pth')\n",
        "            print(f\"  âœ“ Best model saved! Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XwmDBpfF7FSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train c3d model\n",
        "model = train_c3d('/content/basketball_actions', epochs=30)"
      ],
      "metadata": {
        "id": "91DaZX1qCjyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def preprocess_clip(frames, size=(112, 112)):\n",
        "    \"\"\"\n",
        "    frames: list of np.ndarray (BGR)\n",
        "    returns tensor (1, 3, T, H, W)\n",
        "    \"\"\"\n",
        "    clip = []\n",
        "    for frame in frames:\n",
        "        frame = cv2.resize(frame, size)\n",
        "        frame = frame[:, :, ::-1]  # BGR â†’ RGB\n",
        "        clip.append(frame)\n",
        "\n",
        "    clip = np.stack(clip)              # (T, H, W, 3)\n",
        "    clip = clip.transpose(3, 0, 1, 2)  # (3, T, H, W)\n",
        "    clip = torch.tensor(clip).float() / 255.0\n",
        "    return clip.unsqueeze(0)\n",
        "\n",
        "def find_shooter(players, ball):\n",
        "    # the closest player to a ball is a shooter\n",
        "    if ball is None or len(players) == 0:\n",
        "        return None\n",
        "\n",
        "    bx, by = ball[\"center\"]\n",
        "    distances = []\n",
        "\n",
        "    for p in players:\n",
        "        px, py = p[\"center\"]\n",
        "        distances.append((np.hypot(px - bx, py - by), p)) # Euclidean distance to the ball\n",
        "\n",
        "    distances.sort(key=lambda x: x[0])\n",
        "    return distances[0][1]\n",
        "\n",
        "# Combines YOLO-based object detection and C3D-based action recognition\n",
        "# to produce an annotated video with player/ball/basket bounding boxes\n",
        "# and predicted basketball actions (e.g. dunk, lay-up, jump-shot).\n",
        "def process_video(\n",
        "    video_path,\n",
        "    input_path,\n",
        "    output_path,\n",
        "    yolo_model,\n",
        "    c3d_model,\n",
        "    c3d_classes,\n",
        "    clip_len=16\n",
        "):\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path) # opens the input video\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    out = cv2.VideoWriter(\n",
        "        output_path,\n",
        "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "        fps,\n",
        "        (w, h)\n",
        "    ) # Creates the output video writer with same resolution & FPS\n",
        "\n",
        "    frame_buffer = []\n",
        "    action_buffer = []  # stores action label per frame\n",
        "\n",
        "    c3d_model.eval() # puts C3D into inference mode\n",
        "    frame_idx = 1\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        # ret == True -> a frame was read correctly\n",
        "        # ret == False -> no frame was read (end of video or error)\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # -------- YOLO --------\n",
        "        # detections = yolo_model(frame)[0]\n",
        "        txt_file = fr\"{input_path}_{frame_idx}.txt\"\n",
        "        detections = load_yolo_txt(txt_file, w, h)\n",
        "\n",
        "        players, ball, basket = [], None, None\n",
        "\n",
        "        frame_idx += 1\n",
        "        for det in detections:\n",
        "          cls = det[\"cls\"]\n",
        "          conf = det[\"conf\"]\n",
        "          x1, y1, x2, y2 = det[\"bbox\"]\n",
        "          cx, cy = det[\"center\"]\n",
        "\n",
        "          label = yolo_model.names[int(cls)]\n",
        "\n",
        "          obj = {\n",
        "          \"bbox\": (x1, y1, x2, y2),\n",
        "          \"center\": (cx, cy),\n",
        "          \"conf\": conf\n",
        "    }\n",
        "\n",
        "          if label in [\"player\", \"person\"]:\n",
        "              players.append(obj)\n",
        "          elif label in [\"basket\", \"rim\", \"hoop\"]:\n",
        "              ball = obj\n",
        "          elif label == \"basket\":\n",
        "              basket = obj\n",
        "\n",
        "        # -------- C3D clip handling --------\n",
        "        frame_buffer.append(frame.copy())\n",
        "\n",
        "        if len(frame_buffer) == clip_len:\n",
        "            clip = preprocess_clip(frame_buffer)\n",
        "            with torch.no_grad():\n",
        "                logits = c3d_model(clip)\n",
        "                action_id = logits.argmax(dim=1).item()\n",
        "                action_label = c3d_classes[action_id]\n",
        "\n",
        "            action_buffer = [action_label] * clip_len\n",
        "            frame_buffer.clear()\n",
        "\n",
        "        current_action = action_buffer.pop(0) if action_buffer else None\n",
        "\n",
        "        # -------- Drawing --------\n",
        "        for p in players:\n",
        "            x1, y1, x2, y2 = p[\"bbox\"]\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        if ball:\n",
        "            x1, y1, x2, y2 = ball[\"bbox\"]\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "\n",
        "        if basket:\n",
        "            x1, y1, x2, y2 = basket[\"bbox\"]\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "        if current_action:\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                f\"ACTION: {current_action}\",\n",
        "                (20, 150),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1.2,\n",
        "                (255, 0, 255),\n",
        "                3\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n"
      ],
      "metadata": {
        "id": "VUlPvHq5DEwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get txt file after prediction by the model\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Paths (IMPORTANT for Windows + Jupyter)\n",
        "model_path = r\"/content/best11m.pt\"\n",
        "video_path = r\"/content/vid5.mp4\"\n",
        "\n",
        "# Load model\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# Run inference\n",
        "results = model.predict(\n",
        "    source=video_path,\n",
        "    save=False,          # doesn't save annotated video\n",
        "    save_txt=True,      # saves YOLO txt per frame\n",
        "    save_conf=True,\n",
        "    project=\"runs/detect\",\n",
        "    name=\"inference_vid5\",\n",
        "    exist_ok=True,\n",
        "    line_width=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQrxzDg2Jf4x",
        "outputId": "b9cb2657-f7ee-43cc-87d7-d7d56dd3a6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\n",
            "WARNING âš ï¸ \n",
            "Inference results will accumulate in RAM unless `stream=True` is passed, which can cause out-of-memory errors for large\n",
            "sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/192) /content/vid5.mp4: 384x640 1 person, 1158.4ms\n",
            "video 1/1 (frame 2/192) /content/vid5.mp4: 384x640 1 person, 879.1ms\n",
            "video 1/1 (frame 3/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 694.1ms\n",
            "video 1/1 (frame 4/192) /content/vid5.mp4: 384x640 1 person, 664.8ms\n",
            "video 1/1 (frame 5/192) /content/vid5.mp4: 384x640 1 person, 615.8ms\n",
            "video 1/1 (frame 6/192) /content/vid5.mp4: 384x640 1 person, 622.5ms\n",
            "video 1/1 (frame 7/192) /content/vid5.mp4: 384x640 1 person, 724.1ms\n",
            "video 1/1 (frame 8/192) /content/vid5.mp4: 384x640 1 person, 636.4ms\n",
            "video 1/1 (frame 9/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 635.7ms\n",
            "video 1/1 (frame 10/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 683.9ms\n",
            "video 1/1 (frame 11/192) /content/vid5.mp4: 384x640 1 person, 683.5ms\n",
            "video 1/1 (frame 12/192) /content/vid5.mp4: 384x640 2 persons, 698.1ms\n",
            "video 1/1 (frame 13/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 656.7ms\n",
            "video 1/1 (frame 14/192) /content/vid5.mp4: 384x640 2 persons, 678.0ms\n",
            "video 1/1 (frame 15/192) /content/vid5.mp4: 384x640 2 persons, 698.5ms\n",
            "video 1/1 (frame 16/192) /content/vid5.mp4: 384x640 2 persons, 714.9ms\n",
            "video 1/1 (frame 17/192) /content/vid5.mp4: 384x640 2 persons, 680.8ms\n",
            "video 1/1 (frame 18/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 851.7ms\n",
            "video 1/1 (frame 19/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 833.4ms\n",
            "video 1/1 (frame 20/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 643.6ms\n",
            "video 1/1 (frame 21/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 613.6ms\n",
            "video 1/1 (frame 22/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 642.5ms\n",
            "video 1/1 (frame 23/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 709.8ms\n",
            "video 1/1 (frame 24/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 660.9ms\n",
            "video 1/1 (frame 25/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 662.6ms\n",
            "video 1/1 (frame 26/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 690.9ms\n",
            "video 1/1 (frame 27/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 671.3ms\n",
            "video 1/1 (frame 28/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 705.9ms\n",
            "video 1/1 (frame 29/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 646.3ms\n",
            "video 1/1 (frame 30/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 619.2ms\n",
            "video 1/1 (frame 31/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 652.8ms\n",
            "video 1/1 (frame 32/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 654.3ms\n",
            "video 1/1 (frame 33/192) /content/vid5.mp4: 384x640 2 persons, 694.1ms\n",
            "video 1/1 (frame 34/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 683.2ms\n",
            "video 1/1 (frame 35/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 1 tennis racket, 948.8ms\n",
            "video 1/1 (frame 36/192) /content/vid5.mp4: 384x640 2 persons, 1 tennis racket, 758.3ms\n",
            "video 1/1 (frame 37/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 633.2ms\n",
            "video 1/1 (frame 38/192) /content/vid5.mp4: 384x640 2 persons, 633.5ms\n",
            "video 1/1 (frame 39/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 1 tennis racket, 807.6ms\n",
            "video 1/1 (frame 40/192) /content/vid5.mp4: 384x640 1 person, 862.1ms\n",
            "video 1/1 (frame 41/192) /content/vid5.mp4: 384x640 1 person, 1988.8ms\n",
            "video 1/1 (frame 42/192) /content/vid5.mp4: 384x640 1 person, 2192.3ms\n",
            "video 1/1 (frame 43/192) /content/vid5.mp4: 384x640 1 person, 713.7ms\n",
            "video 1/1 (frame 44/192) /content/vid5.mp4: 384x640 1 person, 659.5ms\n",
            "video 1/1 (frame 45/192) /content/vid5.mp4: 384x640 1 person, 655.8ms\n",
            "video 1/1 (frame 46/192) /content/vid5.mp4: 384x640 1 person, 1 tennis racket, 779.1ms\n",
            "video 1/1 (frame 47/192) /content/vid5.mp4: 384x640 1 person, 831.8ms\n",
            "video 1/1 (frame 48/192) /content/vid5.mp4: 384x640 1 person, 631.6ms\n",
            "video 1/1 (frame 49/192) /content/vid5.mp4: 384x640 1 person, 691.4ms\n",
            "video 1/1 (frame 50/192) /content/vid5.mp4: 384x640 1 person, 619.4ms\n",
            "video 1/1 (frame 51/192) /content/vid5.mp4: 384x640 1 person, 639.2ms\n",
            "video 1/1 (frame 52/192) /content/vid5.mp4: 384x640 1 person, 668.1ms\n",
            "video 1/1 (frame 53/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 701.0ms\n",
            "video 1/1 (frame 54/192) /content/vid5.mp4: 384x640 1 person, 703.2ms\n",
            "video 1/1 (frame 55/192) /content/vid5.mp4: 384x640 1 person, 663.5ms\n",
            "video 1/1 (frame 56/192) /content/vid5.mp4: 384x640 1 person, 643.1ms\n",
            "video 1/1 (frame 57/192) /content/vid5.mp4: 384x640 1 person, 637.9ms\n",
            "video 1/1 (frame 58/192) /content/vid5.mp4: 384x640 1 person, 630.1ms\n",
            "video 1/1 (frame 59/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 641.1ms\n",
            "video 1/1 (frame 60/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 630.5ms\n",
            "video 1/1 (frame 61/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 620.7ms\n",
            "video 1/1 (frame 62/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 645.0ms\n",
            "video 1/1 (frame 63/192) /content/vid5.mp4: 384x640 1 person, 847.8ms\n",
            "video 1/1 (frame 64/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 816.7ms\n",
            "video 1/1 (frame 65/192) /content/vid5.mp4: 384x640 1 person, 662.4ms\n",
            "video 1/1 (frame 66/192) /content/vid5.mp4: 384x640 1 person, 690.9ms\n",
            "video 1/1 (frame 67/192) /content/vid5.mp4: 384x640 1 person, 645.3ms\n",
            "video 1/1 (frame 68/192) /content/vid5.mp4: 384x640 1 person, 685.5ms\n",
            "video 1/1 (frame 69/192) /content/vid5.mp4: 384x640 2 persons, 602.9ms\n",
            "video 1/1 (frame 70/192) /content/vid5.mp4: 384x640 1 person, 637.6ms\n",
            "video 1/1 (frame 71/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 659.1ms\n",
            "video 1/1 (frame 72/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 652.2ms\n",
            "video 1/1 (frame 73/192) /content/vid5.mp4: 384x640 2 persons, 662.7ms\n",
            "video 1/1 (frame 74/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 661.4ms\n",
            "video 1/1 (frame 75/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 624.0ms\n",
            "video 1/1 (frame 76/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 1 tennis racket, 639.7ms\n",
            "video 1/1 (frame 77/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 641.5ms\n",
            "video 1/1 (frame 78/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 1 tennis racket, 643.2ms\n",
            "video 1/1 (frame 79/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 664.9ms\n",
            "video 1/1 (frame 80/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 936.2ms\n",
            "video 1/1 (frame 81/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 787.1ms\n",
            "video 1/1 (frame 82/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 1 tennis racket, 689.9ms\n",
            "video 1/1 (frame 83/192) /content/vid5.mp4: 384x640 4 persons, 1 sports ball, 1 tennis racket, 674.4ms\n",
            "video 1/1 (frame 84/192) /content/vid5.mp4: 384x640 5 persons, 635.0ms\n",
            "video 1/1 (frame 85/192) /content/vid5.mp4: 384x640 4 persons, 710.0ms\n",
            "video 1/1 (frame 86/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 789.7ms\n",
            "video 1/1 (frame 87/192) /content/vid5.mp4: 384x640 4 persons, 1 sports ball, 701.2ms\n",
            "video 1/1 (frame 88/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 703.0ms\n",
            "video 1/1 (frame 89/192) /content/vid5.mp4: 384x640 1 person, 649.3ms\n",
            "video 1/1 (frame 90/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 1 tennis racket, 625.5ms\n",
            "video 1/1 (frame 91/192) /content/vid5.mp4: 384x640 1 person, 1 sports ball, 616.4ms\n",
            "video 1/1 (frame 92/192) /content/vid5.mp4: 384x640 1 person, 645.2ms\n",
            "video 1/1 (frame 93/192) /content/vid5.mp4: 384x640 1 person, 701.7ms\n",
            "video 1/1 (frame 94/192) /content/vid5.mp4: 384x640 2 persons, 631.9ms\n",
            "video 1/1 (frame 95/192) /content/vid5.mp4: 384x640 3 persons, 635.5ms\n",
            "video 1/1 (frame 96/192) /content/vid5.mp4: 384x640 2 persons, 816.0ms\n",
            "video 1/1 (frame 97/192) /content/vid5.mp4: 384x640 2 persons, 815.6ms\n",
            "video 1/1 (frame 98/192) /content/vid5.mp4: 384x640 2 persons, 677.1ms\n",
            "video 1/1 (frame 99/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 671.9ms\n",
            "video 1/1 (frame 100/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 629.2ms\n",
            "video 1/1 (frame 101/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 634.1ms\n",
            "video 1/1 (frame 102/192) /content/vid5.mp4: 384x640 2 persons, 636.8ms\n",
            "video 1/1 (frame 103/192) /content/vid5.mp4: 384x640 2 persons, 699.4ms\n",
            "video 1/1 (frame 104/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 663.9ms\n",
            "video 1/1 (frame 105/192) /content/vid5.mp4: 384x640 2 persons, 1 tennis racket, 626.8ms\n",
            "video 1/1 (frame 106/192) /content/vid5.mp4: 384x640 2 persons, 744.9ms\n",
            "video 1/1 (frame 107/192) /content/vid5.mp4: 384x640 2 persons, 1304.3ms\n",
            "video 1/1 (frame 108/192) /content/vid5.mp4: 384x640 2 persons, 907.5ms\n",
            "video 1/1 (frame 109/192) /content/vid5.mp4: 384x640 2 persons, 1 tennis racket, 667.1ms\n",
            "video 1/1 (frame 110/192) /content/vid5.mp4: 384x640 2 persons, 1 tennis racket, 673.3ms\n",
            "video 1/1 (frame 111/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 905.1ms\n",
            "video 1/1 (frame 112/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 745.3ms\n",
            "video 1/1 (frame 113/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 690.6ms\n",
            "video 1/1 (frame 114/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 668.1ms\n",
            "video 1/1 (frame 115/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 711.6ms\n",
            "video 1/1 (frame 116/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 683.7ms\n",
            "video 1/1 (frame 117/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 666.3ms\n",
            "video 1/1 (frame 118/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 629.0ms\n",
            "video 1/1 (frame 119/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 634.4ms\n",
            "video 1/1 (frame 120/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 634.0ms\n",
            "video 1/1 (frame 121/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 633.4ms\n",
            "video 1/1 (frame 122/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 634.5ms\n",
            "video 1/1 (frame 123/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 647.5ms\n",
            "video 1/1 (frame 124/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 693.6ms\n",
            "video 1/1 (frame 125/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 724.4ms\n",
            "video 1/1 (frame 126/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 829.1ms\n",
            "video 1/1 (frame 127/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 942.4ms\n",
            "video 1/1 (frame 128/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 895.7ms\n",
            "video 1/1 (frame 129/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 661.2ms\n",
            "video 1/1 (frame 130/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 698.1ms\n",
            "video 1/1 (frame 131/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 1295.0ms\n",
            "video 1/1 (frame 132/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 1316.8ms\n",
            "video 1/1 (frame 133/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 1076.4ms\n",
            "video 1/1 (frame 134/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 981.6ms\n",
            "video 1/1 (frame 135/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 771.9ms\n",
            "video 1/1 (frame 136/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 763.7ms\n",
            "video 1/1 (frame 137/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 774.8ms\n",
            "video 1/1 (frame 138/192) /content/vid5.mp4: 384x640 2 persons, 761.9ms\n",
            "video 1/1 (frame 139/192) /content/vid5.mp4: 384x640 2 persons, 804.5ms\n",
            "video 1/1 (frame 140/192) /content/vid5.mp4: 384x640 2 persons, 882.3ms\n",
            "video 1/1 (frame 141/192) /content/vid5.mp4: 384x640 2 persons, 1 tennis racket, 819.8ms\n",
            "video 1/1 (frame 142/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 1 tennis racket, 647.7ms\n",
            "video 1/1 (frame 143/192) /content/vid5.mp4: 384x640 2 persons, 699.1ms\n",
            "video 1/1 (frame 144/192) /content/vid5.mp4: 384x640 2 persons, 679.8ms\n",
            "video 1/1 (frame 145/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 770.0ms\n",
            "video 1/1 (frame 146/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 1 tennis racket, 665.5ms\n",
            "video 1/1 (frame 147/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 682.8ms\n",
            "video 1/1 (frame 148/192) /content/vid5.mp4: 384x640 2 persons, 1 tennis racket, 681.8ms\n",
            "video 1/1 (frame 149/192) /content/vid5.mp4: 384x640 2 persons, 707.4ms\n",
            "video 1/1 (frame 150/192) /content/vid5.mp4: 384x640 2 persons, 716.6ms\n",
            "video 1/1 (frame 151/192) /content/vid5.mp4: 384x640 2 persons, 699.4ms\n",
            "video 1/1 (frame 152/192) /content/vid5.mp4: 384x640 2 persons, 736.4ms\n",
            "video 1/1 (frame 153/192) /content/vid5.mp4: 384x640 2 persons, 771.9ms\n",
            "video 1/1 (frame 154/192) /content/vid5.mp4: 384x640 2 persons, 729.8ms\n",
            "video 1/1 (frame 155/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 742.7ms\n",
            "video 1/1 (frame 156/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 940.9ms\n",
            "video 1/1 (frame 157/192) /content/vid5.mp4: 384x640 2 persons, 786.4ms\n",
            "video 1/1 (frame 158/192) /content/vid5.mp4: 384x640 2 persons, 721.6ms\n",
            "video 1/1 (frame 159/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 680.7ms\n",
            "video 1/1 (frame 160/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 685.8ms\n",
            "video 1/1 (frame 161/192) /content/vid5.mp4: 384x640 3 persons, 729.5ms\n",
            "video 1/1 (frame 162/192) /content/vid5.mp4: 384x640 2 persons, 721.9ms\n",
            "video 1/1 (frame 163/192) /content/vid5.mp4: 384x640 2 persons, 702.8ms\n",
            "video 1/1 (frame 164/192) /content/vid5.mp4: 384x640 2 persons, 711.8ms\n",
            "video 1/1 (frame 165/192) /content/vid5.mp4: 384x640 2 persons, 770.7ms\n",
            "video 1/1 (frame 166/192) /content/vid5.mp4: 384x640 2 persons, 700.5ms\n",
            "video 1/1 (frame 167/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 680.4ms\n",
            "video 1/1 (frame 168/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 736.5ms\n",
            "video 1/1 (frame 169/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 843.2ms\n",
            "video 1/1 (frame 170/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 1939.9ms\n",
            "video 1/1 (frame 171/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 992.1ms\n",
            "video 1/1 (frame 172/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 739.5ms\n",
            "video 1/1 (frame 173/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 733.0ms\n",
            "video 1/1 (frame 174/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 724.0ms\n",
            "video 1/1 (frame 175/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 835.0ms\n",
            "video 1/1 (frame 176/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 853.7ms\n",
            "video 1/1 (frame 177/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 710.8ms\n",
            "video 1/1 (frame 178/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 654.6ms\n",
            "video 1/1 (frame 179/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 712.9ms\n",
            "video 1/1 (frame 180/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 918.3ms\n",
            "video 1/1 (frame 181/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 647.0ms\n",
            "video 1/1 (frame 182/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 658.6ms\n",
            "video 1/1 (frame 183/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 655.2ms\n",
            "video 1/1 (frame 184/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 667.5ms\n",
            "video 1/1 (frame 185/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 812.6ms\n",
            "video 1/1 (frame 186/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 1010.6ms\n",
            "video 1/1 (frame 187/192) /content/vid5.mp4: 384x640 2 persons, 1 sports ball, 763.9ms\n",
            "video 1/1 (frame 188/192) /content/vid5.mp4: 384x640 3 persons, 1 sports ball, 688.8ms\n",
            "video 1/1 (frame 189/192) /content/vid5.mp4: 384x640 3 persons, 710.1ms\n",
            "video 1/1 (frame 190/192) /content/vid5.mp4: 384x640 2 persons, 755.8ms\n",
            "video 1/1 (frame 191/192) /content/vid5.mp4: 384x640 3 persons, 741.2ms\n",
            "video 1/1 (frame 192/192) /content/vid5.mp4: 384x640 4 persons, 720.5ms\n",
            "Speed: 3.8ms preprocess, 746.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1m/content/runs/detect/inference_vid5\u001b[0m\n",
            "192 labels saved to /content/runs/detect/inference_vid5/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yolo_txt(txt_path, img_w, img_h):\n",
        "    detections = []\n",
        "\n",
        "    if not os.path.exists(txt_path):\n",
        "        return detections\n",
        "\n",
        "    with open(txt_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            cls, xc, yc, w, h, conf = map(float, line.split())\n",
        "\n",
        "            xc *= img_w\n",
        "            yc *= img_h\n",
        "            w  *= img_w\n",
        "            h  *= img_h\n",
        "\n",
        "            x1 = int(xc - w / 2)\n",
        "            y1 = int(yc - h / 2)\n",
        "            x2 = int(xc + w / 2)\n",
        "            y2 = int(yc + h / 2)\n",
        "\n",
        "            detections.append({\n",
        "                \"cls\": int(cls),\n",
        "                \"conf\": conf,\n",
        "                \"bbox\": (x1, y1, x2, y2),\n",
        "                \"center\": (int(xc), int(yc))\n",
        "            })\n",
        "\n",
        "    return detections\n"
      ],
      "metadata": {
        "id": "LlB0qbBpJ9EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "C3D_CLASSES  = [\"dunk\", \"jump-shot\", \"lay-up\"]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "video_path = '/content/vid5.mp4'\n",
        "labels_player_path = '/content/runs/detect/inference_vid5/labels/vid5'\n",
        "output_path = '/content/output.mp4'\n",
        "yolo_model = YOLO('/content/best11m.pt')\n",
        "\n",
        "c3d_device = torch.device(\"cpu\")\n",
        "\n",
        "c3d_model = C3D(num_classes=3)\n",
        "c3d_model.load_state_dict(\n",
        "    torch.load(\"c3d_basketball_best.pth\", map_location=c3d_device)\n",
        ")\n",
        "c3d_model.to(c3d_device)\n",
        "c3d_model.eval()\n",
        "\n",
        "\n",
        "process_video(\n",
        "    video_path,\n",
        "    labels_player_path,\n",
        "    output_path,\n",
        "    yolo_model,\n",
        "    c3d_model,\n",
        "    C3D_CLASSES\n",
        ")"
      ],
      "metadata": {
        "id": "TELRJVyuECnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I3D\n"
      ],
      "metadata": {
        "id": "cIg3c-_tyBuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pytorchvideo"
      ],
      "metadata": {
        "id": "bb-unNy5_X-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5f9bbe-6884-465a-f2f4-14ec7331524d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/132.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m \u001b[32m122.9/132.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, time\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from torchvision.transforms import v2 as T\n",
        "from pytorchvideo.models.hub import i3d_r50\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "DATASET_ROOT = \"dataset\"  # dataset/train, dataset/val, dataset/test\n",
        "\n",
        "CLASS_NAMES = [\"layup\", \"dunk\", \"jump_shoot\"]\n",
        "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "CLIP_LEN = 32\n",
        "IMG_SIZE = 224\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "NUM_WORKERS = 0\n",
        "ACCUM_STEPS = 4\n",
        "\n",
        "WARMUP_EPOCHS = 2\n",
        "FINETUNE_EPOCHS = 8\n",
        "TOTAL_EPOCHS = WARMUP_EPOCHS + FINETUNE_EPOCHS\n",
        "\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/I3D_RUNB_CHECKPOINTS\"\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIN = (device == \"cuda\")\n",
        "print(\"device:\", device)\n"
      ],
      "metadata": {
        "id": "vlh45Ndo_T44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eebc7d7-6663-435e-f118-bbd2073281db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_video(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    if len(frames) == 0:\n",
        "        raise RuntimeError(f\"Empty video: {path}\")\n",
        "    return np.stack(frames)  # [T,H,W,C]\n",
        "\n",
        "def clip_indices(num_frames, clip_len, train=True):\n",
        "    if num_frames >= clip_len:\n",
        "        start = random.randint(0, num_frames - clip_len) if train else (num_frames - clip_len)//2\n",
        "        return torch.arange(start, start + clip_len)\n",
        "    idx = torch.arange(num_frames)\n",
        "    pad = idx[-1].repeat(clip_len - num_frames)\n",
        "    return torch.cat([idx, pad])\n",
        "\n",
        "def list_videos(split_dir):\n",
        "    split_dir = Path(split_dir)\n",
        "    samples = []\n",
        "    for cls in CLASS_NAMES:\n",
        "        cls_dir = split_dir / cls\n",
        "        if not cls_dir.exists():\n",
        "            raise FileNotFoundError(f\"Missing folder: {cls_dir}\")\n",
        "        for p in cls_dir.rglob(\"*\"):\n",
        "            if p.suffix.lower() in [\".mp4\", \".avi\", \".mov\", \".mkv\"]:\n",
        "                samples.append((str(p), CLASS_TO_IDX[cls]))\n",
        "    if len(samples) == 0:\n",
        "        raise RuntimeError(f\"No videos in {split_dir}\")\n",
        "    return samples\n"
      ],
      "metadata": {
        "id": "Kv0clJPbFnZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, split_dir, train=True):\n",
        "        self.samples = list_videos(split_dir)\n",
        "        self.train = train\n",
        "\n",
        "        if train:\n",
        "            self.tf = T.Compose([\n",
        "                T.ToDtype(torch.float32, scale=True),\n",
        "                T.Resize((256,256)),\n",
        "                T.RandomCrop((IMG_SIZE, IMG_SIZE)),\n",
        "                T.RandomHorizontalFlip(0.5),\n",
        "                T.ColorJitter(0.2,0.2,0.2,0.05),\n",
        "                T.Normalize((0.45,0.45,0.45), (0.225,0.225,0.225)),\n",
        "            ])\n",
        "        else:\n",
        "            self.tf = T.Compose([\n",
        "                T.ToDtype(torch.float32, scale=True),\n",
        "                T.Resize((256,256)),\n",
        "                T.CenterCrop((IMG_SIZE, IMG_SIZE)),\n",
        "                T.Normalize((0.45,0.45,0.45), (0.225,0.225,0.225)),\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, y = self.samples[idx]\n",
        "        vid = torch.from_numpy(read_video(path))          # [T,H,W,C]\n",
        "        ids = clip_indices(vid.shape[0], CLIP_LEN, self.train)\n",
        "\n",
        "        clip = vid[ids].permute(0,3,1,2)                  # [T,C,H,W]\n",
        "        clip = self.tf(clip).permute(1,0,2,3)             # [C,T,H,W]\n",
        "        return clip, y\n",
        "\n",
        "train_ds = VideoDataset(os.path.join(DATASET_ROOT, \"train\"), train=True)\n",
        "val_ds   = VideoDataset(os.path.join(DATASET_ROOT, \"val\"),   train=False)\n",
        "test_ds  = VideoDataset(os.path.join(DATASET_ROOT, \"test\"),  train=False)\n",
        "\n",
        "# counts\n",
        "labels = [y for _,y in train_ds.samples]\n",
        "counts = np.bincount(labels, minlength=NUM_CLASSES).astype(np.float32)\n",
        "print(\"train counts:\", counts, \"order:\", CLASS_NAMES)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=PIN, drop_last=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=PIN)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=PIN)\n",
        "\n",
        "x0, y0 = next(iter(train_loader))\n",
        "print(\"batch:\", x0.shape, y0[:8])\n"
      ],
      "metadata": {
        "id": "KfLH-7IKFvbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a93282-5c95-4e3d-acf8-8183c72acdde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train counts: [1296.   92. 1280.] order: ['layup', 'dunk', 'jump_shoot']\n",
            "batch: torch.Size([2, 3, 32, 224, 224]) tensor([0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = i3d_r50(pretrained=True)\n",
        "model.blocks[-1].proj = nn.Linear(model.blocks[-1].proj.in_features, NUM_CLASSES)\n",
        "model = model.to(device)\n",
        "\n",
        "# class weights (inverse frequency, clipped)\n",
        "w = 1.0 / (counts + 1e-6)\n",
        "w = np.clip(w, 0, 10.0)\n",
        "w = w / w.sum() * NUM_CLASSES\n",
        "class_weights = torch.tensor(w, device=device, dtype=torch.float32)\n",
        "print(\"CE weights:\", class_weights.detach().cpu().numpy())\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device == \"cuda\"))\n",
        "\n",
        "def set_warmup_freeze(model):\n",
        "    for n,p in model.named_parameters():\n",
        "        p.requires_grad = (\"proj\" in n)\n",
        "\n",
        "def set_finetune_unfreeze(model):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "set_warmup_freeze(model)\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "7VCsri5nFykq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fafc5cb-5a21-4032-8f55-c9a6833639c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/I3D_8x8_R50.pyth\" to /root/.cache/torch/hub/checkpoints/I3D_8x8_R50.pyth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214M/214M [00:01<00:00, 114MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE weights: [0.18634167 2.6249871  0.18867096]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ckpt(model, epoch, best_val, path):\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"best_val\": best_val,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"class_names\": CLASS_NAMES,\n",
        "        \"clip_len\": CLIP_LEN,\n",
        "        \"img_size\": IMG_SIZE\n",
        "    }, path)\n",
        "\n",
        "def drive_paths(epoch):\n",
        "    best = os.path.join(DRIVE_DIR, \"best_i3d_runB.pt\")\n",
        "    last = os.path.join(DRIVE_DIR, \"last_i3d_runB.pt\")\n",
        "    ep   = os.path.join(DRIVE_DIR, f\"epoch_{epoch:02d}_i3d_runB.pt\")\n",
        "    return best, last, ep\n",
        "\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    for step, (xb, yb) in enumerate(train_loader, 1):\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb) / ACCUM_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if step % ACCUM_STEPS == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        total_loss += loss.item() * xb.size(0) * ACCUM_STEPS\n",
        "        correct += (logits.argmax(1) == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "\n",
        "        if step % 20 == 0:\n",
        "            print(f\"epoch {epoch:02d} step {step:04d} | \"\n",
        "                  f\"loss {(loss.item()*ACCUM_STEPS):.4f} | \"\n",
        "                  f\"avg {(time.time()-t0)/step:.3f}s/step\", flush=True)\n",
        "\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loader(loader):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        correct += (logits.argmax(1) == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "best_val = 0.0\n",
        "\n",
        "for epoch in range(1, TOTAL_EPOCHS+1):\n",
        "    print(f\"\\n===== Epoch {epoch}/{TOTAL_EPOCHS} =====\", flush=True)\n",
        "\n",
        "    if epoch == WARMUP_EPOCHS + 1:\n",
        "        print(\"ğŸ”“ Unfreeze backbone (finetune)\")\n",
        "        set_finetune_unfreeze(model)\n",
        "        optimizer = AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "\n",
        "    tr_loss, tr_acc = train_one_epoch(epoch)\n",
        "    va_loss, va_acc = eval_loader(val_loader)\n",
        "\n",
        "    phase = \"WARMUP\" if epoch <= WARMUP_EPOCHS else \"FINETUNE\"\n",
        "    print(f\"[{phase}] train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
        "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\", flush=True)\n",
        "\n",
        "    best_path, last_path, ep_path = drive_paths(epoch)\n",
        "    save_ckpt(model, epoch, best_val, last_path)\n",
        "    save_ckpt(model, epoch, best_val, ep_path)\n",
        "\n",
        "    if va_acc > best_val:\n",
        "        best_val = va_acc\n",
        "        save_ckpt(model, epoch, best_val, best_path)\n",
        "        print(f\"âœ… saved BEST: {best_path} (val acc {best_val:.3f})\", flush=True)\n",
        "\n",
        "print(\"\\nğŸ Done. Best val acc:\", best_val)\n"
      ],
      "metadata": {
        "id": "_7Z_PYrYF1TJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "385bd83e-88ea-41fd-c536-18b029f3d3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Epoch 1/10 =====\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3660940877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mva_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3660940877.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2670112591.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# [T,C,H,W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# [C,T,H,W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mneeds_unpacking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneeds_unpacking\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         flat_outputs = [\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneeds_transform\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_transform_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_color.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, inpt, params)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_saturation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaturation_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfn_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhue_factor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_hue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_transform.py\u001b[0m in \u001b[0;36m_call_kernel\u001b[0;34m(self, functional, inpt, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_passthrough\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/functional/_color.py\u001b[0m in \u001b[0;36madjust_hue_image\u001b[0;34m(image, hue_factor)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhue_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremainder_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mimage_hue_adj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_hsv_to_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mto_dtype_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_hue_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/functional/_color.py\u001b[0m in \u001b[0;36m_hsv_to_rgb\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mselect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvpqt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test_report(ckpt_path):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    model.eval()\n",
        "    print(\"Loaded:\", ckpt_path, \"| epoch:\", ckpt[\"epoch\"], \"| best_val:\", ckpt[\"best_val\"])\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "            probs = F.softmax(model(xb), dim=1)\n",
        "        pred = probs.argmax(1).cpu().numpy()\n",
        "        y_true.append(yb.numpy())\n",
        "        y_pred.append(pred)\n",
        "\n",
        "    y_true = np.concatenate(y_true)\n",
        "    y_pred = np.concatenate(y_pred)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    cm  = confusion_matrix(y_true, y_pred)\n",
        "    rep = classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=3)\n",
        "\n",
        "    print(\"\\nâœ… TEST acc:\", acc)\n",
        "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "    print(\"\\nReport:\\n\", rep)\n",
        "\n",
        "best_path = os.path.join(DRIVE_DIR, \"best_i3d.pt\")\n",
        "test_report(best_path)\n"
      ],
      "metadata": {
        "id": "XyrhhE_HF6sQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb56e362-869a-443c-bade-453e9a5138de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: /content/drive/MyDrive/I3D_RUNB_CHECKPOINTS/best_i3d.pt | epoch: 3 | best_val: 0.6666666666666666\n",
            "\n",
            "âœ… TEST acc: 0.48823529411764705\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 28  15 197]\n",
            " [  0  19   1]\n",
            " [ 29  19 202]]\n",
            "\n",
            "Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       layup      0.491     0.117     0.189       240\n",
            "        dunk      0.358     0.950     0.521        20\n",
            "  jump_shoot      0.505     0.808     0.622       250\n",
            "\n",
            "    accuracy                          0.488       510\n",
            "   macro avg      0.452     0.625     0.444       510\n",
            "weighted avg      0.493     0.488     0.414       510\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== DEMO: YOLO detections + I3D shot-type on real video (ONE CELL) ======\n",
        "\n",
        "import os, cv2, time\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import v2 as T\n",
        "from pytorchvideo.models.hub import i3d_r50\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ------------------ PATHS (CHANGE THESE) ------------------\n",
        "VIDEO_PATH = \"/content/vid3.mp4\"  # <<< your real video\n",
        "YOLO_WEIGHTS = \"/content/best.pt\" # <<< your YOLO weights (objects)\n",
        "I3D_BEST_CKPT = \"/content/drive/MyDrive/I3D_RUNB_CHECKPOINTS/best_i3d.pt\"  # <<< your I3D best\n",
        "\n",
        "OUTPUT_PATH = \"output_demo.mp4\"\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "CLASS_NAMES = [\"layup\", \"dunk\", \"jump_shoot\"]\n",
        "CLIP_LEN = 32\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# YOLO classes you want to keep (set to None to keep everything)\n",
        "KEEP_CLASSES = [\n",
        "    \"ball\", \"ball-in-basket\", \"player-jump-shot\",\n",
        "    \"player-layup-dunk\", \"player-shot-block\",\n",
        "    \"player-in-possession\", \"rim\"\n",
        "]\n",
        "CONF_THRES = 0.25\n",
        "IOU_THRES = 0.45\n",
        "\n",
        "# I3D smoothing (to stabilize prediction)\n",
        "SMOOTH_WINDOW = 12  # increase => more stable, but slower reaction\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "\n",
        "# ------------------ LOAD YOLO ------------------\n",
        "yolo = YOLO(YOLO_WEIGHTS)\n",
        "yolo_names = yolo.names  # id->name\n",
        "keep_ids = None\n",
        "if KEEP_CLASSES is not None:\n",
        "    keep_ids = set([i for i, n in yolo_names.items() if n in KEEP_CLASSES])\n",
        "    print(\"YOLO keep IDs:\", keep_ids)\n",
        "\n",
        "# ------------------ LOAD I3D ------------------\n",
        "i3d = i3d_r50(pretrained=True)\n",
        "i3d.blocks[-1].proj = nn.Linear(i3d.blocks[-1].proj.in_features, len(CLASS_NAMES))\n",
        "i3d = i3d.to(device)\n",
        "\n",
        "ckpt = torch.load(I3D_BEST_CKPT, map_location=device)\n",
        "state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
        "i3d.load_state_dict(state, strict=True)\n",
        "i3d.eval()\n",
        "print(\"Loaded I3D checkpoint:\", I3D_BEST_CKPT)\n",
        "\n",
        "# transforms for I3D (match your val/test pipeline)\n",
        "tf = T.Compose([\n",
        "    T.ToDtype(torch.float32, scale=True),\n",
        "    T.Resize((256, 256)),\n",
        "    T.CenterCrop((IMG_SIZE, IMG_SIZE)),\n",
        "    T.Normalize(mean=(0.45,0.45,0.45), std=(0.225,0.225,0.225)),\n",
        "])\n",
        "\n",
        "# ------------------ VIDEO IO ------------------\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "if not cap.isOpened():\n",
        "    raise RuntimeError(\"Cannot open video: \" + VIDEO_PATH)\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "if fps is None or fps <= 0:\n",
        "    fps = 25.0\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (W, H))\n",
        "\n",
        "# ------------------ BUFFERS ------------------\n",
        "clip_buffer = deque(maxlen=CLIP_LEN)   # store RGB frames for I3D\n",
        "pred_buffer = deque(maxlen=SMOOTH_WINDOW)\n",
        "\n",
        "def draw_label(img_bgr, text, x, y, scale=0.8, thickness=2):\n",
        "    cv2.putText(img_bgr, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, scale, (0,0,0), thickness+3, cv2.LINE_AA)\n",
        "    cv2.putText(img_bgr, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, scale, (255,255,255), thickness, cv2.LINE_AA)\n",
        "\n",
        "def i3d_predict_from_buffer(buffer_rgb):\n",
        "    \"\"\"\n",
        "    buffer_rgb: list length CLIP_LEN, each frame RGB uint8 [H,W,3]\n",
        "    returns: (pred_id, probs numpy)\n",
        "    \"\"\"\n",
        "    # [T,H,W,C] -> torch [T,C,H,W]\n",
        "    vid = torch.from_numpy(np.stack(buffer_rgb)).permute(0,3,1,2)  # uint8\n",
        "    vid = tf(vid)                      # [T,C,h,w] float normalized\n",
        "    vid = vid.permute(1,0,2,3).unsqueeze(0)  # [1,C,T,h,w]\n",
        "    vid = vid.to(device, non_blocking=True)\n",
        "\n",
        "    with torch.amp.autocast(\"cuda\", enabled=(device==\"cuda\")):\n",
        "        logits = i3d(vid)\n",
        "        probs = F.softmax(logits, dim=1)[0].detach().float().cpu().numpy()\n",
        "    pred = int(np.argmax(probs))\n",
        "    return pred, probs\n",
        "\n",
        "# ------------------ MAIN LOOP ------------------\n",
        "frame_idx = 0\n",
        "t_start = time.time()\n",
        "\n",
        "current_action = \"...\"\n",
        "current_probs = None\n",
        "\n",
        "while True:\n",
        "    ret, frame_bgr = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_idx += 1\n",
        "\n",
        "    # ---- YOLO detect (BGR -> model handles) ----\n",
        "    # (We run on the original frame for boxes)\n",
        "    yres = yolo.predict(frame_bgr, conf=CONF_THRES, iou=IOU_THRES, verbose=False)[0]\n",
        "    boxes = yres.boxes  # xyxy, conf, cls\n",
        "\n",
        "    # ---- draw YOLO boxes ----\n",
        "    if boxes is not None and len(boxes) > 0:\n",
        "        xyxy = boxes.xyxy.cpu().numpy()\n",
        "        conf = boxes.conf.cpu().numpy()\n",
        "        cls  = boxes.cls.cpu().numpy().astype(int)\n",
        "\n",
        "        for (x1,y1,x2,y2), c, k in zip(xyxy, conf, cls):\n",
        "            if keep_ids is not None and k not in keep_ids:\n",
        "                continue\n",
        "            name = yolo_names.get(k, str(k))\n",
        "            x1,y1,x2,y2 = map(int, [x1,y1,x2,y2])\n",
        "            cv2.rectangle(frame_bgr, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "            draw_label(frame_bgr, f\"{name} {c:.2f}\", x1, max(20, y1-8), scale=0.6, thickness=1)\n",
        "\n",
        "    # ---- push RGB frame into clip buffer for I3D ----\n",
        "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "    clip_buffer.append(frame_rgb)\n",
        "\n",
        "    # ---- once buffer full => predict I3D each N frames (optional speed) ----\n",
        "    # predict every 2 frames to reduce compute (change to 1 if you want)\n",
        "    if len(clip_buffer) == CLIP_LEN and (frame_idx % 2 == 0):\n",
        "        pred_id, probs = i3d_predict_from_buffer(list(clip_buffer))\n",
        "        pred_buffer.append(pred_id)\n",
        "\n",
        "        # majority vote smoothing\n",
        "        if len(pred_buffer) > 0:\n",
        "            vals, counts = np.unique(np.array(pred_buffer), return_counts=True)\n",
        "            smoothed_id = int(vals[np.argmax(counts)])\n",
        "            current_action = CLASS_NAMES[smoothed_id]\n",
        "            current_probs = probs\n",
        "\n",
        "    # ---- overlay action + optional probs ----\n",
        "    draw_label(frame_bgr, f\"SHOT TYPE: {current_action}\", 20, 40, scale=1.0, thickness=2)\n",
        "\n",
        "    if current_probs is not None:\n",
        "        # show small probs\n",
        "        ptxt = \" | \".join([f\"{CLASS_NAMES[i]}:{current_probs[i]:.2f}\" for i in range(len(CLASS_NAMES))])\n",
        "        draw_label(frame_bgr, ptxt, 20, 75, scale=0.65, thickness=1)\n",
        "\n",
        "    # ---- write frame ----\n",
        "    writer.write(frame_bgr)\n",
        "\n",
        "cap.release()\n",
        "writer.release()\n",
        "\n",
        "print(f\"âœ… Done. Saved: {OUTPUT_PATH}\")\n",
        "print(\"Frames:\", frame_idx, \"time(s):\", round(time.time()-t_start, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe4FgzC2ORyo",
        "outputId": "c2641eeb-761e-408c-be9c-a74a21560e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "YOLO keep IDs: {0, 1, 4, 5, 6, 7, 9}\n",
            "Loaded I3D checkpoint: /content/drive/MyDrive/I3D_RUNB_CHECKPOINTS/best_i3d.pt\n",
            "âœ… Done. Saved: output_demo.mp4\n",
            "Frames: 121 time(s): 23.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOLO avec regles spatiaux"
      ],
      "metadata": {
        "id": "Pa1BlhI2mSeS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nsnAmnjVdkfb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get(\"ROBOFLOW_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqaIz7RDdmoN",
        "outputId": "acf9fda4-c546-49f6-9b78-4a87cc0a7661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it1h4bvIfT4U",
        "outputId": "d61285f1-c1bc-4ae8-fb6a-4fdf69513830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u2Iyla4lfedz",
        "outputId": "6170b641-bb82-4d25-bf29-6e0c0aceed64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/105.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m102.4/105.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.4/99.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.1/190.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m327.1/327.1 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m280.8/280.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.7/160.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m292.3/292.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m304.1/304.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m300.6/300.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for paho-mqtt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pybase64 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyvips (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "sse-starlette 3.0.4 requires starlette>=0.49.1, but you have starlette 0.46.2 which is incompatible.\n",
            "bigframes 2.30.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
            "google-adk 1.21.0 requires starlette<1.0.0,>=0.49.1, but you have starlette 0.46.2 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\n",
            "esda 2.8.0 requires shapely>=2.1, but you have shapely 2.0.7 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
            "spopt 0.7.0 requires shapely>=2.1.0, but you have shapely 2.0.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sports (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gdown\n",
        "!pip install -q inference-gpu\n",
        "!pip install -q git+https://github.com/roboflow/supervision.git\n",
        "!pip install -q git+https://github.com/roboflow/sports.git@feat/basketball"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rTGhZ2ilNEBe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"ONNXRUNTIME_EXECUTION_PROVIDERS\"] = \"[CUDAExecutionProvider]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "polkWGq4gFzX",
        "outputId": "e577ddcc-20ef-4fe7-dc11-6e98c60a3625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1GYxJGG8_OT5wlHxcYjUb62P2rjHeTg7p boston-celtics-new-york-knicks-game-1-q1-01.54-01.48.mp4\n",
            "Processing file 1Gm-QSkfXngzEnkrQEdE1HsLPG3cU-KCV boston-celtics-new-york-knicks-game-1-q1-03.16-03.11.mp4\n",
            "Processing file 1It3wz3eoGcjo6tI9OGUt3X69ZK6x5SNK boston-celtics-new-york-knicks-game-1-q1-04.28-04.20.mp4\n",
            "Processing file 1hx7_rJAmgBjt6PgHH8wh49OwUh2Lhe9f boston-celtics-new-york-knicks-game-1-q1-04.44-04.39.mp4\n",
            "Processing file 1fPEw_w51nNQeBHh_GYhRyThVg71V1trn boston-celtics-new-york-knicks-game-1-q1-05.13-05.09.mp4\n",
            "Processing file 1zwnAE4jHVI0qH7ioDFnzX98XtOocAUbH boston-celtics-new-york-knicks-game-1-q1-06.00-05.54.mp4\n",
            "Processing file 1wQMSO-C4jOm6BBIVUuJ_6Uqf0QLdbJHD boston-celtics-new-york-knicks-game-1-q1-07.41-07.34.mp4\n",
            "Processing file 1w9e1zCZtXtOmi6C4m-1BJJzTpa3znkbB boston-celtics-new-york-knicks-game-1-q2-08.09-08.03.mp4\n",
            "Processing file 1zaltcB_-j8Pzxh8rBuvAV2o0Ngf3IX0I boston-celtics-new-york-knicks-game-1-q2-08.43-08.38.mp4\n",
            "Processing file 14DuRHherVUn-CmER3z9oOiroWfkbk7eP boston-celtics-new-york-knicks-game-1-q2-10.36-10.32.mp4\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GYxJGG8_OT5wlHxcYjUb62P2rjHeTg7p\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q1-01.54-01.48.mp4\n",
            "100% 8.30M/8.30M [00:00<00:00, 34.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Gm-QSkfXngzEnkrQEdE1HsLPG3cU-KCV\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q1-03.16-03.11.mp4\n",
            "100% 6.60M/6.60M [00:00<00:00, 37.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1It3wz3eoGcjo6tI9OGUt3X69ZK6x5SNK\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q1-04.28-04.20.mp4\n",
            "100% 10.4M/10.4M [00:00<00:00, 34.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hx7_rJAmgBjt6PgHH8wh49OwUh2Lhe9f\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q1-04.44-04.39.mp4\n",
            "100% 6.16M/6.16M [00:00<00:00, 27.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fPEw_w51nNQeBHh_GYhRyThVg71V1trn\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q1-05.13-05.09.mp4\n",
            "100% 5.25M/5.25M [00:00<00:00, 27.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zwnAE4jHVI0qH7ioDFnzX98XtOocAUbH\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q1-06.00-05.54.mp4\n",
            "100% 7.96M/7.96M [00:00<00:00, 26.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wQMSO-C4jOm6BBIVUuJ_6Uqf0QLdbJHD\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q1-07.41-07.34.mp4\n",
            "100% 9.28M/9.28M [00:00<00:00, 43.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1w9e1zCZtXtOmi6C4m-1BJJzTpa3znkbB\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q2-08.09-08.03.mp4\n",
            "100% 7.32M/7.32M [00:00<00:00, 34.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zaltcB_-j8Pzxh8rBuvAV2o0Ngf3IX0I\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q2-08.43-08.38.mp4\n",
            "100% 8.77M/8.77M [00:00<00:00, 28.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14DuRHherVUn-CmER3z9oOiroWfkbk7eP\n",
            "To: /content/boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q2-10.36-10.32.mp4\n",
            "100% 4.76M/4.76M [00:00<00:00, 23.2MB/s]\n",
            "Download completed\n",
            "Retrieving folder contents\n",
            "Processing file 1D2JPj8CxzQT7L2VrrDOmxXX0sqCRUjFU Bayon-Regular.ttf\n",
            "Processing file 1hCpPlFwxhtLLtoeuOhKsjcvXoGsGJEi0 ConcertOne-Regular.ttf\n",
            "Processing file 1PE8V1I42Cd0OS5ApDIpHOQSFxjkLg5aC SquadaOne-Regular.ttf\n",
            "Processing file 13DU0TYTgXtFfO02y-5lgQpzc1PSUA5WE Staatliches-Regular.ttf\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D2JPj8CxzQT7L2VrrDOmxXX0sqCRUjFU\n",
            "To: /content/fonts/Bayon-Regular.ttf\n",
            "100% 54.6k/54.6k [00:00<00:00, 3.23MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hCpPlFwxhtLLtoeuOhKsjcvXoGsGJEi0\n",
            "To: /content/fonts/ConcertOne-Regular.ttf\n",
            "100% 70.0k/70.0k [00:00<00:00, 2.55MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PE8V1I42Cd0OS5ApDIpHOQSFxjkLg5aC\n",
            "To: /content/fonts/SquadaOne-Regular.ttf\n",
            "100% 17.8k/17.8k [00:00<00:00, 43.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13DU0TYTgXtFfO02y-5lgQpzc1PSUA5WE\n",
            "To: /content/fonts/Staatliches-Regular.ttf\n",
            "100% 61.4k/61.4k [00:00<00:00, 2.10MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# videos\n",
        "# !gdown --folder https://drive.google.com/drive/folders/1eDJYqQ77Fytz15tKGdJCMeYSgmoQ-2-H\n",
        "# !gdown --folder https://drive.google.com/drive/folders/1RBjpI5Xleb58lujeusxH0W5zYMMA4ytO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sqjj2VVYRpm9"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "SOURCE_VIDEO_PATH = Path(HOME) / \"boston-celtics-new-york-knicks-game-1/boston-celtics-new-york-knicks-game-1-q1-03.16-03.11.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC4Zs87lQ7K1",
        "outputId": "8344ba82-75c8-4e92-a0a2-8ff5702d37ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM_ENABLED to False.\n",
            "ModelDependencyMissing: Your `inference` configuration does not support SAM2 model. Use pip install 'inference[sam]' to install missing requirements.To suppress this warning, set CORE_MODEL_SAM2_ENABLED to False.\n",
            "ModelDependencyMissing: Your `inference` configuration does not support SAM3 model. Install SAM3 dependencies and set CORE_MODEL_SAM3_ENABLED to True.\n",
            "ModelDependencyMissing: Your `inference` configuration does not support Gaze Detection model. Use pip install 'inference[gaze]' to install missing requirements.To suppress this warning, set CORE_MODEL_GAZE_ENABLED to False.\n",
            "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "ModelDependencyMissing: Your `inference` configuration does not support YoloWorld model. Use pip install 'inference[yolo-world]' to install missing requirements.To suppress this warning, set CORE_MODEL_YOLO_WORLD_ENABLED to False.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import supervision as sv\n",
        "import cv2\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from inference import get_model\n",
        "from google.colab import userdata\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from collections import deque\n",
        "from typing import Union, Sequence, Optional, List\n",
        "\n",
        "from sports import MeasurementUnit, ViewTransformer\n",
        "from sports.basketball import (\n",
        "    CourtConfiguration,\n",
        "    League,\n",
        "    draw_court,\n",
        "    draw_made_and_miss_on_court,\n",
        "    ShotEventTracker,\n",
        "    ShotEvent,\n",
        "    ShotType\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AblicOIONoLW",
        "outputId": "19b5529a-9a09-4bfa-e245-56a5f584ecf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolved model_id: basketball-player-detection-3-ycjdo/4, dataset_id: basketball-player-detection-3-ycjdo, version_id: 4\n"
          ]
        }
      ],
      "source": [
        "PLAYER_DETECTION_MODEL_ID = \"basketball-player-detection-3-ycjdo/4\"\n",
        "PLAYER_DETECTION_MODEL = get_model(model_id=PLAYER_DETECTION_MODEL_ID, api_key =  userdata.get(\"ROBOFLOW_API_KEY\"))\n",
        "\n",
        "COLOR = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\",\n",
        "    \"#b266ff\", \"#9999ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_1KPaIFSAbp",
        "outputId": "a3adf92a-db4b-4abd-b922-86cb65113f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolved model_id: basketball-court-detection-2/14, dataset_id: basketball-court-detection-2, version_id: 14\n"
          ]
        }
      ],
      "source": [
        "COURT_DETECTION_MODEL_ID = \"basketball-court-detection-2/14\"\n",
        "COURT_DETECTION_MODEL = get_model(model_id=COURT_DETECTION_MODEL_ID, api_key =  userdata.get(\"ROBOFLOW_API_KEY\"))\n",
        "\n",
        "MAGENTA_COLOR = sv.Color.from_hex('#FF1493')\n",
        "CYAN_COLOR = sv.Color.from_hex('#00BFFF')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MyOrhBrHTVv1"
      },
      "outputs": [],
      "source": [
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.7\n",
        "\n",
        "BALL_IN_BASKET_CLASS_ID = 1\n",
        "JUMP_SHOT_CLASS_ID = 5\n",
        "LAYUP_DUNK_CLASS_ID = 6\n",
        "\n",
        "COLOR = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff66ff\", \"#3399ff\", \"#ff66b2\", \"#ff8080\",\n",
        "    \"#b266ff\", \"#9999ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(color=COLOR, thickness=2)\n",
        "label_annotator = sv.LabelAnnotator(color=COLOR, text_color=sv.Color.BLACK)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F4N62eQQgGm_"
      },
      "outputs": [],
      "source": [
        "CONFIG = CourtConfiguration(league=League.NBA, measurement_unit=MeasurementUnit.FEET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MaXkpN5gLKl",
        "outputId": "64e962d1-35d9-4e63-81bf-0eb21522041c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Font path '/content/fonts/Staatliches-Regular.ttf' not found. Using PIL's default font.\n",
            "Font path '/content/fonts/Staatliches-Regular.ttf' not found. Using PIL's default font.\n"
          ]
        }
      ],
      "source": [
        "COLOR = sv.ColorPalette.from_hex(['#007A33', '#006BB6'])\n",
        "TEXT_COLOR = sv.Color.WHITE\n",
        "\n",
        "triangle_annotator = sv.TriangleAnnotator(\n",
        "    color=COLOR,\n",
        "    base=25,\n",
        "    height=21,\n",
        "    color_lookup=sv.ColorLookup.CLASS\n",
        ")\n",
        "text_annotator = sv.RichLabelAnnotator(\n",
        "    font_path=f\"{HOME}/fonts/Staatliches-Regular.ttf\",\n",
        "    font_size=60,\n",
        "    color=COLOR,\n",
        "    text_color=TEXT_COLOR,\n",
        "    text_offset=(0, -30),\n",
        "    color_lookup=sv.ColorLookup.CLASS,\n",
        "    text_position=sv.Position.TOP_CENTER\n",
        ")\n",
        "\n",
        "triangle_annotator_missed = sv.TriangleAnnotator(\n",
        "    color=sv.Color.from_hex(\"#850101\"),\n",
        "    base=25,\n",
        "    height=21,\n",
        "    color_lookup=sv.ColorLookup.CLASS\n",
        ")\n",
        "text_annotator_missed = sv.RichLabelAnnotator(\n",
        "    font_path=f\"{HOME}/fonts/Staatliches-Regular.ttf\",\n",
        "    font_size=60,\n",
        "    color=sv.Color.from_hex(\"#850101\"),\n",
        "    text_color=TEXT_COLOR,\n",
        "    text_offset=(0, -30),\n",
        "    color_lookup=sv.ColorLookup.CLASS,\n",
        "    text_position=sv.Position.TOP_CENTER\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EEw5r8UJh-QX"
      },
      "outputs": [],
      "source": [
        "class KeyPointsSmoother:\n",
        "    def __init__(self, length: int):\n",
        "        self.length = length\n",
        "        self.buffer = deque(maxlen=length)\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        xy: np.ndarray,\n",
        "        confidence: Optional[np.ndarray] = None,\n",
        "        conf_threshold: float = 0.0,\n",
        "    ) -> np.ndarray:\n",
        "        assert xy.ndim == 3 and xy.shape[0] == 1\n",
        "        xy_f = xy.astype(np.float32, copy=True)\n",
        "\n",
        "        if confidence is not None:\n",
        "            assert confidence.shape[:2] == xy.shape[:2]\n",
        "            mask = (confidence >= conf_threshold)[..., None]\n",
        "            xy_f = np.where(mask, xy_f, np.nan)\n",
        "\n",
        "        self.buffer.append(xy_f)\n",
        "        stacked = np.stack(self.buffer, axis=0)\n",
        "\n",
        "        if np.isnan(stacked).any():\n",
        "            mean_xy = np.nanmean(stacked, axis=0)\n",
        "        else:\n",
        "            mean_xy = stacked.mean(axis=0)\n",
        "\n",
        "        return mean_xy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HiWK8VTQgVm0"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Shot:\n",
        "    x: float\n",
        "    y: float\n",
        "    distance: float\n",
        "    result: bool\n",
        "    team: int\n",
        "    shot_type: str = \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MjmRubCYTnBw"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(\n",
        "    start_point: Union[Sequence[float], np.ndarray],\n",
        "    end_point: Union[Sequence[float], np.ndarray]\n",
        ") -> float:\n",
        "    start_point_array = np.asarray(start_point, dtype=float)\n",
        "    end_point_array = np.asarray(end_point, dtype=float)\n",
        "\n",
        "    if start_point_array.shape != (2,) or end_point_array.shape != (2,):\n",
        "        raise ValueError(\"Both points must have shape (2,).\")\n",
        "\n",
        "    return float(np.linalg.norm(end_point_array - start_point_array))\n",
        "\n",
        "def extract_made(shots: list[Shot]):\n",
        "    return [shot for shot in shots if shot.result]\n",
        "\n",
        "def extract_xy(shots: list[Shot]):\n",
        "    return np.array([[shot.x, shot.y] for shot in shots], dtype=float)\n",
        "\n",
        "def extract_class_id(shots: list[Shot]):\n",
        "    return np.array([shot.team for shot in shots], dtype=int)\n",
        "\n",
        "def extract_label(shots: list[Shot]):\n",
        "    return np.array([f\"{shot.distance:.2f} ft\" for shot in shots], dtype=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Dc9YI4ZDyPjg"
      },
      "outputs": [],
      "source": [
        "def classify_layup_or_dunk(player_detection, basket_position, frame):\n",
        "    \"\"\"\n",
        "    Distinguish layup from dunk using heuristics\n",
        "    \"\"\"\n",
        "    # Get player bounding box\n",
        "    x1, y1, x2, y2 = player_detection.xyxy[0]\n",
        "    player_top = y1\n",
        "    player_height = y2 - y1\n",
        "\n",
        "    # Estimate basket rim height in image coordinates\n",
        "    basket_y = basket_position[1]  # You'd get this from court keypoints\n",
        "\n",
        "    # Heuristic 1: Height relative to basket\n",
        "    # If player's top is significantly above basket rim, likely a dunk\n",
        "    height_above_basket = basket_y - player_top\n",
        "\n",
        "    if height_above_basket > player_height * 0.3:  # Adjust threshold\n",
        "        return \"dunk\"\n",
        "    else:\n",
        "        return \"layup\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZwHeNnglhoMV"
      },
      "outputs": [],
      "source": [
        "SOURCE_VIDEO_DIR = Path(HOME) #/ \"boston-celtics-new-york-knicks-game-1\"\n",
        "\n",
        "SOURCE_VIDEO_NAMES = [\n",
        "    \"jumpshoot.mp4\",\n",
        "]\n",
        "\n",
        "SOURCE_VIDEO_PATHS = [\n",
        "    SOURCE_VIDEO_DIR / video_name\n",
        "    for video_name\n",
        "    in SOURCE_VIDEO_NAMES\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xe41A2tHOFDa"
      },
      "outputs": [],
      "source": [
        "# Add these constants at the top with your other configurations\n",
        "SLOW_MOTION_FACTOR = 0.3  # 30% speed (slower = lower value)\n",
        "SLOW_MOTION_DURATION_SECONDS = 2  # Duration of slow-mo effect\n",
        "ZOOM_FACTOR = 1.5  # 1.5x zoom on player\n",
        "ZOOM_TRANSITION_FRAMES = 15  # Smooth zoom transition\n",
        "\n",
        "@dataclass\n",
        "class AnimationState:\n",
        "    \"\"\"Track animation state for slow-motion and zoom effects\"\"\"\n",
        "    is_animating: bool = False\n",
        "    animation_start_frame: int = 0\n",
        "    animation_type: str = \"\"  # \"shot_start\", \"shot_made\", \"shot_missed\"\n",
        "    target_bbox: Optional[np.ndarray] = None  # Player bbox to zoom on\n",
        "    zoom_level: float = 1.0\n",
        "    playback_speed: float = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "E_g0YAGnPg-6"
      },
      "outputs": [],
      "source": [
        "def calculate_zoom_transform(\n",
        "    bbox: np.ndarray,\n",
        "    frame_shape: tuple,\n",
        "    zoom_factor: float,\n",
        "    current_zoom: float,\n",
        "    transition_frames: int,\n",
        "    frame_in_transition: int\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Calculate smooth zoom transformation centered on player bbox.\n",
        "\n",
        "    Returns:\n",
        "        (zoom_level, crop_coords) where crop_coords = (x1, y1, x2, y2)\n",
        "    \"\"\"\n",
        "    # Smooth interpolation for zoom\n",
        "    progress = min(frame_in_transition / transition_frames, 1.0)\n",
        "    # Ease-in-out function for smooth animation\n",
        "    smooth_progress = progress * progress * (3 - 2 * progress)\n",
        "    target_zoom = 1.0 + (zoom_factor - 1.0) * smooth_progress\n",
        "\n",
        "    frame_h, frame_w = frame_shape[:2]\n",
        "\n",
        "    # Calculate center of player bbox\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    center_x = (x1 + x2) / 2\n",
        "    center_y = (y1 + y2) / 2\n",
        "\n",
        "    # Calculate crop region for zoom\n",
        "    crop_w = frame_w / target_zoom\n",
        "    crop_h = frame_h / target_zoom\n",
        "\n",
        "    # Center crop on player\n",
        "    crop_x1 = max(0, center_x - crop_w / 2)\n",
        "    crop_y1 = max(0, center_y - crop_h / 2)\n",
        "    crop_x2 = min(frame_w, crop_x1 + crop_w)\n",
        "    crop_y2 = min(frame_h, crop_y1 + crop_h)\n",
        "\n",
        "    # Adjust if we hit frame boundaries\n",
        "    if crop_x2 - crop_x1 < crop_w:\n",
        "        crop_x1 = max(0, crop_x2 - crop_w)\n",
        "    if crop_y2 - crop_y1 < crop_h:\n",
        "        crop_y1 = max(0, crop_y2 - crop_h)\n",
        "\n",
        "    return target_zoom, (int(crop_x1), int(crop_y1), int(crop_x2), int(crop_y2))\n",
        "\n",
        "\n",
        "def apply_zoom_effect(\n",
        "    frame: np.ndarray,\n",
        "    crop_coords: tuple,\n",
        "    target_size: tuple\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply zoom effect by cropping and resizing.\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = crop_coords\n",
        "    cropped = frame[y1:y2, x1:x2]\n",
        "\n",
        "    # Resize back to original frame size\n",
        "    zoomed = cv2.resize(cropped, (target_size[1], target_size[0]),\n",
        "                        interpolation=cv2.INTER_LINEAR)\n",
        "    return zoomed\n",
        "\n",
        "\n",
        "def add_slow_motion_indicator(\n",
        "    frame: np.ndarray,\n",
        "    slow_motion_active: bool,\n",
        "    playback_speed: float\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Add visual indicator for slow-motion effect.\n",
        "    \"\"\"\n",
        "    if not slow_motion_active:\n",
        "        return frame\n",
        "\n",
        "    # Add \"SLOW MOTION\" text\n",
        "    text = f\"SLOW-MO {int(playback_speed * 100)}%\"\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 0.8\n",
        "    thickness = 2\n",
        "\n",
        "    text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
        "\n",
        "    # Position at bottom-right\n",
        "    margin = 20\n",
        "    x = frame.shape[1] - text_size[0] - margin\n",
        "    y = frame.shape[0] - margin\n",
        "\n",
        "    # Semi-transparent background\n",
        "    overlay = frame.copy()\n",
        "    cv2.rectangle(\n",
        "        overlay,\n",
        "        (x - 10, y - text_size[1] - 10),\n",
        "        (x + text_size[0] + 10, y + 10),\n",
        "        (0, 0, 0),\n",
        "        -1\n",
        "    )\n",
        "    frame = cv2.addWeighted(overlay, 0.5, frame, 0.5, 0)\n",
        "\n",
        "    # Text\n",
        "    cv2.putText(\n",
        "        frame,\n",
        "        text,\n",
        "        (x, y),\n",
        "        font,\n",
        "        font_scale,\n",
        "        (255, 255, 0),  # Cyan color\n",
        "        thickness,\n",
        "        cv2.LINE_AA\n",
        "    )\n",
        "\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_Qu909xiHxR",
        "outputId": "3a7d133a-9e34-4ee6-be84-0ad42e24d3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Videos:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Frames: jumpshoot.mp4:   0%|          | 0/181 [00:00<?, ?it/s]\u001b[ARuntimeWarning: Mean of empty slice\n",
            "\n",
            "Frames: jumpshoot.mp4:   1%|          | 1/181 [00:06<18:20,  6.11s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   1%|          | 2/181 [00:10<14:21,  4.81s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   2%|â–         | 3/181 [00:13<12:35,  4.24s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   2%|â–         | 4/181 [00:17<11:41,  3.97s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   3%|â–         | 5/181 [00:21<12:09,  4.14s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   3%|â–         | 6/181 [00:25<11:28,  3.93s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   4%|â–         | 7/181 [00:28<11:04,  3.82s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   4%|â–         | 8/181 [00:33<11:36,  4.02s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   5%|â–         | 9/181 [00:36<11:07,  3.88s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   6%|â–Œ         | 10/181 [00:40<10:44,  3.77s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   6%|â–Œ         | 11/181 [00:44<11:10,  3.94s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   7%|â–‹         | 12/181 [00:48<10:57,  3.89s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   7%|â–‹         | 13/181 [00:51<10:33,  3.77s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   8%|â–Š         | 14/181 [00:55<10:24,  3.74s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   8%|â–Š         | 15/181 [00:59<10:48,  3.91s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   9%|â–‰         | 16/181 [01:03<10:26,  3.80s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:   9%|â–‰         | 17/181 [01:06<10:09,  3.71s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  10%|â–‰         | 18/181 [01:11<10:40,  3.93s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  10%|â–ˆ         | 19/181 [01:14<10:17,  3.81s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  11%|â–ˆ         | 20/181 [01:18<10:01,  3.74s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  12%|â–ˆâ–        | 21/181 [01:22<10:27,  3.92s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  12%|â–ˆâ–        | 22/181 [01:26<10:35,  4.00s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  13%|â–ˆâ–        | 23/181 [01:30<10:09,  3.86s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  13%|â–ˆâ–        | 24/181 [01:34<10:19,  3.95s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  14%|â–ˆâ–        | 25/181 [01:38<10:12,  3.92s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  14%|â–ˆâ–        | 26/181 [01:42<09:49,  3.80s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  15%|â–ˆâ–        | 27/181 [01:45<09:36,  3.74s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  15%|â–ˆâ–Œ        | 28/181 [01:50<10:04,  3.95s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  16%|â–ˆâ–Œ        | 29/181 [01:53<09:39,  3.81s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  17%|â–ˆâ–‹        | 30/181 [01:57<09:21,  3.72s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  17%|â–ˆâ–‹        | 31/181 [02:01<09:51,  3.94s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  18%|â–ˆâ–Š        | 32/181 [02:05<09:28,  3.82s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  18%|â–ˆâ–Š        | 33/181 [02:08<09:13,  3.74s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  19%|â–ˆâ–‰        | 34/181 [02:12<09:24,  3.84s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  19%|â–ˆâ–‰        | 35/181 [02:16<09:23,  3.86s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  20%|â–ˆâ–‰        | 36/181 [02:20<09:05,  3.76s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  20%|â–ˆâ–ˆ        | 37/181 [02:23<08:50,  3.69s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  21%|â–ˆâ–ˆ        | 38/181 [02:28<09:19,  3.91s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  22%|â–ˆâ–ˆâ–       | 39/181 [02:31<08:58,  3.80s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  22%|â–ˆâ–ˆâ–       | 40/181 [02:35<08:45,  3.72s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  23%|â–ˆâ–ˆâ–       | 41/181 [02:39<09:13,  3.96s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  23%|â–ˆâ–ˆâ–       | 42/181 [02:43<08:52,  3.83s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  24%|â–ˆâ–ˆâ–       | 43/181 [02:46<08:35,  3.74s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  24%|â–ˆâ–ˆâ–       | 44/181 [02:50<08:53,  3.89s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  25%|â–ˆâ–ˆâ–       | 45/181 [02:55<08:56,  3.94s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  25%|â–ˆâ–ˆâ–Œ       | 46/181 [02:59<09:09,  4.07s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  26%|â–ˆâ–ˆâ–Œ       | 47/181 [03:03<09:12,  4.12s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  27%|â–ˆâ–ˆâ–‹       | 48/181 [03:07<08:55,  4.03s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  27%|â–ˆâ–ˆâ–‹       | 49/181 [03:10<08:33,  3.89s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  28%|â–ˆâ–ˆâ–Š       | 50/181 [03:14<08:21,  3.83s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  28%|â–ˆâ–ˆâ–Š       | 51/181 [03:19<08:38,  3.99s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  29%|â–ˆâ–ˆâ–Š       | 52/181 [03:22<08:17,  3.86s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  29%|â–ˆâ–ˆâ–‰       | 53/181 [03:26<08:01,  3.76s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  30%|â–ˆâ–ˆâ–‰       | 54/181 [03:30<08:23,  3.97s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  30%|â–ˆâ–ˆâ–ˆ       | 55/181 [03:34<08:03,  3.84s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  31%|â–ˆâ–ˆâ–ˆ       | 56/181 [03:37<07:49,  3.75s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  31%|â–ˆâ–ˆâ–ˆâ–      | 57/181 [03:42<08:24,  4.07s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  32%|â–ˆâ–ˆâ–ˆâ–      | 58/181 [03:45<08:00,  3.91s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  33%|â–ˆâ–ˆâ–ˆâ–      | 59/181 [03:49<07:42,  3.79s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  33%|â–ˆâ–ˆâ–ˆâ–      | 60/181 [03:53<07:44,  3.84s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  34%|â–ˆâ–ˆâ–ˆâ–      | 61/181 [03:57<07:48,  3.90s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  34%|â–ˆâ–ˆâ–ˆâ–      | 62/181 [04:01<07:33,  3.81s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  35%|â–ˆâ–ˆâ–ˆâ–      | 63/181 [04:04<07:19,  3.72s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 64/181 [04:09<07:41,  3.95s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 65/181 [04:12<07:24,  3.83s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 66/181 [04:16<07:11,  3.75s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 67/181 [04:20<07:30,  3.95s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 68/181 [04:24<07:11,  3.82s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 69/181 [04:27<06:57,  3.73s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 70/181 [04:31<07:05,  3.83s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 71/181 [04:35<07:03,  3.85s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 72/181 [04:39<06:49,  3.76s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 73/181 [04:42<06:38,  3.69s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 74/181 [04:47<06:57,  3.91s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 75/181 [04:50<06:41,  3.78s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 76/181 [04:54<06:28,  3.70s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 77/181 [04:58<06:47,  3.92s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78/181 [05:02<06:34,  3.83s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79/181 [05:05<06:21,  3.74s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80/181 [05:09<06:32,  3.88s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 81/181 [05:13<06:24,  3.84s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 82/181 [05:17<06:10,  3.74s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 83/181 [05:20<06:02,  3.70s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 84/181 [05:25<06:16,  3.88s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 85/181 [05:28<06:02,  3.77s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 86/181 [05:32<05:50,  3.69s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 87/181 [05:36<06:07,  3.91s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 88/181 [05:40<05:53,  3.80s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 89/181 [05:43<05:42,  3.72s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 90/181 [05:47<05:53,  3.89s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 91/181 [05:51<05:46,  3.84s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 92/181 [05:55<05:34,  3.75s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 93/181 [05:58<05:29,  3.74s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 94/181 [06:03<05:39,  3.90s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 95/181 [06:06<05:26,  3.80s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 96/181 [06:10<05:16,  3.72s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 97/181 [06:14<05:29,  3.93s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98/181 [06:18<05:16,  3.81s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99/181 [06:21<05:04,  3.72s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 100/181 [06:26<05:17,  3.92s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 101/181 [06:29<05:04,  3.80s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 102/181 [06:33<04:52,  3.70s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 103/181 [06:36<04:51,  3.73s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 104/181 [06:41<04:58,  3.88s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 105/181 [06:44<04:47,  3.78s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 106/181 [06:48<04:37,  3.70s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 107/181 [06:52<04:48,  3.90s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 108/181 [06:56<04:36,  3.78s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 109/181 [06:59<04:27,  3.71s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 110/181 [07:04<04:40,  3.94s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 111/181 [07:07<04:27,  3.83s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 112/181 [07:11<04:18,  3.75s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 113/181 [07:15<04:19,  3.82s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 114/181 [07:19<04:19,  3.88s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 115/181 [07:22<04:09,  3.78s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 116/181 [07:26<04:00,  3.71s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 117/181 [07:30<04:11,  3.94s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 118/181 [07:34<04:01,  3.84s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 119/181 [07:37<03:53,  3.77s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 120/181 [07:42<04:02,  3.98s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 121/181 [07:45<03:51,  3.85s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 122/181 [07:49<03:42,  3.76s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 123/181 [07:54<03:51,  3.99s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 124/181 [07:59<04:03,  4.28s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 125/181 [08:02<03:48,  4.09s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 126/181 [08:06<03:41,  4.03s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 127/181 [08:10<03:41,  4.10s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 128/181 [08:14<03:29,  3.95s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 129/181 [08:18<03:19,  3.85s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 130/181 [08:22<03:26,  4.05s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 131/181 [08:26<03:15,  3.91s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 132/181 [08:29<03:06,  3.81s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 133/181 [08:34<03:12,  4.01s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 134/181 [08:37<03:02,  3.88s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135/181 [08:41<02:54,  3.80s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 136/181 [08:45<02:58,  3.97s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 137/181 [08:49<02:51,  3.90s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 138/181 [08:53<02:43,  3.81s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 139/181 [08:56<02:40,  3.81s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 140/181 [09:01<02:41,  3.94s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 141/181 [09:04<02:33,  3.83s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/181 [09:08<02:26,  3.76s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 143/181 [09:12<02:30,  3.97s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 144/181 [09:16<02:22,  3.85s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 145/181 [09:19<02:15,  3.77s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 146/181 [09:24<02:19,  3.99s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 147/181 [09:27<02:11,  3.86s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 148/181 [09:31<02:04,  3.78s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 149/181 [09:35<02:05,  3.93s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 150/181 [09:39<02:00,  3.89s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 151/181 [09:43<01:54,  3.80s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 152/181 [09:46<01:49,  3.76s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 153/181 [09:52<02:04,  4.46s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 154/181 [09:57<01:59,  4.41s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 155/181 [10:01<01:51,  4.28s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 156/181 [10:05<01:46,  4.28s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 157/181 [10:09<01:37,  4.07s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 158/181 [10:12<01:30,  3.92s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 159/181 [10:17<01:29,  4.09s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 160/181 [10:20<01:22,  3.93s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 161/181 [10:24<01:16,  3.81s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 162/181 [10:28<01:16,  4.01s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 163/181 [10:32<01:09,  3.88s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 164/181 [10:35<01:04,  3.78s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 165/181 [10:40<01:02,  3.93s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 166/181 [10:43<00:58,  3.88s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 167/181 [10:47<00:52,  3.78s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 168/181 [10:51<00:49,  3.78s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 169/181 [10:55<00:47,  3.93s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 170/181 [10:59<00:42,  3.82s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 171/181 [11:02<00:37,  3.76s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 172/181 [11:07<00:36,  4.02s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 173/181 [11:10<00:31,  3.90s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 174/181 [11:14<00:26,  3.83s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 175/181 [11:19<00:24,  4.06s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 176/181 [11:22<00:19,  3.92s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 177/181 [11:26<00:15,  3.82s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 178/181 [11:30<00:11,  3.99s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 179/181 [11:34<00:07,  3.92s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 180/181 [11:38<00:03,  3.83s/it]\u001b[A\n",
            "Frames: jumpshoot.mp4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181/181 [11:41<00:00,  3.83s/it]\u001b[A\n",
            "Videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:42<00:00, 702.06s/it]\n"
          ]
        }
      ],
      "source": [
        "KEYPOINT_CONFIDENCE_THRESHOLD = 0.5\n",
        "DETECTION_CONFIDENCE = 0.3\n",
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.7\n",
        "\n",
        "BALL_IN_BASKET_CLASS_ID = 1\n",
        "JUMP_SHOT_CLASS_ID = 5\n",
        "# LAYUP_CLASS_ID = 6\n",
        "# DUNK_CLASS_ID = 7\n",
        "LAYUP_DUNK_CLASS_ID = 6\n",
        "\n",
        "BALL_IN_BASKET_MIN_CONSECUTIVE_FRAMES = 2\n",
        "JUMP_SHOT_MIN_CONSECUTIVE_FRAMES = 3\n",
        "LAYUP_DUNK_MIN_CONSECUTIVE_FRAMES = 3\n",
        "\n",
        "shots: List[Shot] = []\n",
        "\n",
        "COURT_SCALE = 20\n",
        "COURT_PADDING = 50\n",
        "COURT_LINE_THICKNESS = 4\n",
        "court_base = draw_court(\n",
        "    config=CONFIG,\n",
        "    scale=COURT_SCALE,\n",
        "    padding=COURT_PADDING,\n",
        "    line_thickness=COURT_LINE_THICKNESS,\n",
        ")\n",
        "court_h, court_w = court_base.shape[:2]\n",
        "\n",
        "animation_state = AnimationState()\n",
        "frames_buffer = []  # Buffer to duplicate frames for slow-motion\n",
        "info_display_end_frame = None\n",
        "\n",
        "for video_path in tqdm(SOURCE_VIDEO_PATHS, desc=\"Videos\", position=0):\n",
        "\n",
        "    target_video_path = (\n",
        "        video_path.parent / f\"{video_path.stem}-markers{video_path.suffix}\"\n",
        "    )\n",
        "    target_video_compressed_path = (\n",
        "        target_video_path.parent\n",
        "        / f\"{target_video_path.stem}-compressed{target_video_path.suffix}\"\n",
        "    )\n",
        "\n",
        "    target_court_video_path = (\n",
        "        video_path.parent / f\"{video_path.stem}-court{video_path.suffix}\"\n",
        "    )\n",
        "    target_court_video_compressed_path = (\n",
        "        target_court_video_path.parent\n",
        "        / f\"{target_court_video_path.stem}-compressed\"\n",
        "        f\"{target_court_video_path.suffix}\"\n",
        "    )\n",
        "\n",
        "    video_info = sv.VideoInfo.from_video_path(str(video_path))\n",
        "    total_frames = getattr(\n",
        "        video_info, \"total_frames\", getattr(video_info, \"frame_count\", None)\n",
        "    )\n",
        "    frame_generator = sv.get_video_frames_generator(str(video_path))\n",
        "\n",
        "    court_video_info = sv.VideoInfo(\n",
        "        width=court_w,\n",
        "        height=court_h,\n",
        "        fps=video_info.fps,\n",
        "        total_frames=total_frames,\n",
        "    )\n",
        "\n",
        "    shot_event_tracker = ShotEventTracker(\n",
        "        reset_time_frames=int(video_info.fps * 1.7),\n",
        "        minimum_frames_between_starts=int(video_info.fps * 0.5),\n",
        "        cooldown_frames_after_made=int(video_info.fps * 0.5),\n",
        "    )\n",
        "\n",
        "    smoother = KeyPointsSmoother(length=3)\n",
        "    shot_in_progress_xy: Optional[np.ndarray] = None\n",
        "    shot_in_progress_type: str = \"unknown\"\n",
        "    current_shot_distance = None\n",
        "    current_shot_result = None\n",
        "\n",
        "    with sv.VideoSink(str(target_video_path), video_info) as sink, \\\n",
        "         sv.VideoSink(str(target_court_video_path), court_video_info) as court_sink:\n",
        "\n",
        "        for frame_index, frame in tqdm(\n",
        "            enumerate(frame_generator),\n",
        "            total=int(total_frames) if total_frames else None,\n",
        "            desc=f\"Frames: {video_path.name}\",\n",
        "            position=1,\n",
        "            leave=False,\n",
        "        ):\n",
        "    # ================= Player detections and state ================\n",
        "\n",
        "          player_result = PLAYER_DETECTION_MODEL.infer(\n",
        "              frame,\n",
        "              confidence=CONFIDENCE_THRESHOLD,\n",
        "              iou_threshold=IOU_THRESHOLD,\n",
        "          )[0]\n",
        "          player_detections = sv.Detections.from_inference(player_result)\n",
        "\n",
        "          has_jump_shot = (\n",
        "              len(player_detections[\n",
        "                  player_detections.class_id == JUMP_SHOT_CLASS_ID\n",
        "              ]) > 0\n",
        "          )\n",
        "          has_layup_dunk = (\n",
        "              len(player_detections[\n",
        "                  player_detections.class_id == LAYUP_DUNK_CLASS_ID\n",
        "              ]) > 0\n",
        "          )\n",
        "          has_ball_in_basket = (\n",
        "              len(player_detections[\n",
        "                  player_detections.class_id == BALL_IN_BASKET_CLASS_ID\n",
        "              ]) > 0\n",
        "          )\n",
        "\n",
        "          events = shot_event_tracker.update(\n",
        "              frame_index=frame_index,\n",
        "              has_jump_shot=has_jump_shot,\n",
        "              has_layup_dunk=has_layup_dunk,\n",
        "              has_ball_in_basket=has_ball_in_basket,\n",
        "          )\n",
        "\n",
        "          # ================= Court keypoints and transforms =============\n",
        "\n",
        "          court_result = COURT_DETECTION_MODEL.infer(\n",
        "              frame, confidence=DETECTION_CONFIDENCE\n",
        "          )[0]\n",
        "          key_points = sv.KeyPoints.from_inference(court_result)\n",
        "          key_points.xy = smoother.update(xy=key_points.xy, confidence=key_points.confidence, conf_threshold=0.5)\n",
        "\n",
        "          key_mask = key_points.confidence[0] > KEYPOINT_CONFIDENCE_THRESHOLD\n",
        "          have_enough_points = np.count_nonzero(key_mask) >= 4\n",
        "\n",
        "          if have_enough_points:\n",
        "              court_vertices_masked = np.array(CONFIG.vertices)[key_mask]\n",
        "              detected_on_image = key_points[:, key_mask].xy[0]\n",
        "\n",
        "              image_to_court = ViewTransformer(\n",
        "                  source=detected_on_image,\n",
        "                  target=court_vertices_masked,\n",
        "              )\n",
        "              court_to_image = ViewTransformer(\n",
        "                  source=court_vertices_masked,\n",
        "                  target=detected_on_image,\n",
        "              )\n",
        "\n",
        "          # ================= Events and Animation Triggers =================\n",
        "\n",
        "          if events:\n",
        "              start_events = [e for e in events if e[\"event\"] == \"START\"]\n",
        "              made_events = [e for e in events if e[\"event\"] == \"MADE\"]\n",
        "              missed_events = [e for e in events if e[\"event\"] == \"MISSED\"]\n",
        "\n",
        "              if start_events and have_enough_points:\n",
        "                  anchors_image = player_detections[\n",
        "                      (player_detections.class_id == JUMP_SHOT_CLASS_ID)\n",
        "                      | (player_detections.class_id == LAYUP_DUNK_CLASS_ID)\n",
        "                  ].get_anchors_coordinates(\n",
        "                      anchor=sv.Position.BOTTOM_CENTER\n",
        "                  )\n",
        "                  if len(anchors_image) > 0:\n",
        "                      anchors_court = image_to_court.transform_points(\n",
        "                          points=anchors_image\n",
        "                      )\n",
        "                      shot_in_progress_xy = anchors_court[0]\n",
        "\n",
        "                      basket_xy = CONFIG.vertices[CONFIG.left_basket_index]\n",
        "\n",
        "                      current_shot_distance = euclidean_distance(\n",
        "                          start_point=shot_in_progress_xy,\n",
        "                          end_point=basket_xy\n",
        "                      )\n",
        "\n",
        "                      current_shot_result = None\n",
        "\n",
        "                      shot_detections = player_detections[\n",
        "                          (player_detections.class_id == JUMP_SHOT_CLASS_ID)\n",
        "                          | (player_detections.class_id == LAYUP_DUNK_CLASS_ID)\n",
        "                      ]\n",
        "\n",
        "                      detected_class = shot_detections.class_id[0]\n",
        "\n",
        "                      if detected_class == JUMP_SHOT_CLASS_ID:\n",
        "                          shot_in_progress_type = \"jump_shot\"\n",
        "                      else:\n",
        "                          if have_enough_points:\n",
        "                              basket_position_court = CONFIG.vertices[CONFIG.left_basket_index]\n",
        "                              basket_position_image = court_to_image.transform_points(\n",
        "                                  np.array([basket_position_court])\n",
        "                              )[0]\n",
        "                              player_bbox = shot_detections.xyxy[0]\n",
        "                              shot_in_progress_type = classify_layup_or_dunk(\n",
        "                                  player_bbox=player_bbox,\n",
        "                                  basket_position_image=basket_position_image,\n",
        "                                  frame_height=frame.shape[0]\n",
        "                              )\n",
        "                          else:\n",
        "                              shot_in_progress_type = \"layup_dunk\"\n",
        "\n",
        "                      # *** TRIGGER ANIMATION ON SHOT START ***\n",
        "                      animation_state.is_animating = True\n",
        "                      animation_state.animation_start_frame = frame_index\n",
        "                      animation_state.animation_type = \"shot_start\"\n",
        "                      animation_state.target_bbox = shot_detections.xyxy[0]\n",
        "\n",
        "              if made_events and shot_in_progress_xy is not None:\n",
        "                  shots.append(\n",
        "                      Shot(\n",
        "                          x=shot_in_progress_xy[0],\n",
        "                          y=shot_in_progress_xy[1],\n",
        "                          distance=euclidean_distance(\n",
        "                              start_point=shot_in_progress_xy,\n",
        "                              end_point=CONFIG.vertices[CONFIG.left_basket_index],\n",
        "                          ),\n",
        "                          result=True,\n",
        "                          team=0,\n",
        "                          shot_type=shot_in_progress_type,\n",
        "                      )\n",
        "                  )\n",
        "\n",
        "\n",
        "                  current_shot_result = True\n",
        "                  info_display_end_frame = frame_index + int(video_info.fps * 2)\n",
        "\n",
        "\n",
        "\n",
        "                  # *** TRIGGER ANIMATION ON MADE SHOT ***\n",
        "                  animation_state.is_animating = True\n",
        "                  animation_state.animation_start_frame = frame_index\n",
        "                  animation_state.animation_type = \"shot_made\"\n",
        "\n",
        "                  shot_in_progress_xy = None\n",
        "                  shot_in_progress_type = \"unknown\"\n",
        "\n",
        "              if missed_events and shot_in_progress_xy is not None:\n",
        "                  shots.append(\n",
        "                      Shot(\n",
        "                          x=shot_in_progress_xy[0],\n",
        "                          y=shot_in_progress_xy[1],\n",
        "                          distance=euclidean_distance(\n",
        "                              start_point=shot_in_progress_xy,\n",
        "                              end_point=CONFIG.vertices[CONFIG.left_basket_index],\n",
        "                          ),\n",
        "                          result=False,\n",
        "                          team=0,\n",
        "                          shot_type=shot_in_progress_type,\n",
        "                      )\n",
        "                  )\n",
        "                  current_shot_result = False\n",
        "\n",
        "\n",
        "\n",
        "                  shot_in_progress_xy = None\n",
        "                  shot_in_progress_type = \"unknown\"\n",
        "\n",
        "              if info_display_end_frame is not None and frame_index > info_display_end_frame:\n",
        "                  current_shot_distance = None\n",
        "                  current_shot_result = None\n",
        "                  info_display_end_frame = None\n",
        "\n",
        "          # ================= Apply Animation Effects ===================\n",
        "\n",
        "          frames_to_write = [frame.copy()]\n",
        "\n",
        "          if animation_state.is_animating:\n",
        "              frames_elapsed = frame_index - animation_state.animation_start_frame\n",
        "              animation_duration_frames = int(video_info.fps * SLOW_MOTION_DURATION_SECONDS)\n",
        "\n",
        "              if frames_elapsed < animation_duration_frames:\n",
        "                  # Apply zoom effect\n",
        "                  if animation_state.target_bbox is not None:\n",
        "                      zoom_level, crop_coords = calculate_zoom_transform(\n",
        "                          bbox=animation_state.target_bbox,\n",
        "                          frame_shape=frame.shape,\n",
        "                          zoom_factor=ZOOM_FACTOR,\n",
        "                          current_zoom=animation_state.zoom_level,\n",
        "                          transition_frames=ZOOM_TRANSITION_FRAMES,\n",
        "                          frame_in_transition=frames_elapsed\n",
        "                      )\n",
        "\n",
        "                      frame = apply_zoom_effect(\n",
        "                          frame=frame,\n",
        "                          crop_coords=crop_coords,\n",
        "                          target_size=(frame.shape[0], frame.shape[1])\n",
        "                      )\n",
        "                      animation_state.zoom_level = zoom_level\n",
        "\n",
        "                  # Create slow-motion by duplicating frames\n",
        "                  num_duplicates = int(1 / SLOW_MOTION_FACTOR)\n",
        "                  frames_to_write = [frame.copy() for _ in range(num_duplicates)]\n",
        "                  animation_state.playback_speed = SLOW_MOTION_FACTOR\n",
        "              else:\n",
        "                  # Animation finished\n",
        "                  animation_state.is_animating = False\n",
        "                  animation_state.zoom_level = 1.0\n",
        "                  animation_state.playback_speed = 1.0\n",
        "                  animation_state.target_bbox = None\n",
        "                  # current_shot_distance = None\n",
        "                  # current_shot_result = None\n",
        "\n",
        "          # ================= Render broadcast overlay ===================\n",
        "\n",
        "          for frame_to_annotate in frames_to_write:\n",
        "              annotated = frame_to_annotate.copy()\n",
        "              current_shot_text = None\n",
        "\n",
        "              if has_jump_shot:\n",
        "                  current_shot_text = \"JUMP SHOT\"\n",
        "              elif has_layup_dunk:\n",
        "                  if shot_in_progress_type == \"layup\":\n",
        "                      current_shot_text = \"LAYUP\"\n",
        "                  elif shot_in_progress_type == \"dunk\":\n",
        "                      current_shot_text = \"DUNK\"\n",
        "                  else:\n",
        "                      current_shot_text = \"LAYUP/DUNK\"\n",
        "\n",
        "              # Draw the shot type text on screen\n",
        "              if current_shot_text is not None:\n",
        "\n",
        "                  y_offset = 50\n",
        "\n",
        "                  text_size = cv2.getTextSize(\n",
        "                      current_shot_text,\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                      1.5,\n",
        "                      3\n",
        "                  )[0]\n",
        "\n",
        "                  box_height = 40 + text_size[1]\n",
        "\n",
        "                  if current_shot_distance is not None:\n",
        "                      box_height += 40\n",
        "                  if current_shot_result is not None:\n",
        "                      box_height += 40\n",
        "\n",
        "                  cv2.rectangle(\n",
        "                      annotated,\n",
        "                      (40, y_offset - 10),\n",
        "                      (60 + max(text_size[0], 300), y_offset + box_height),\n",
        "                      (0, 0, 0),\n",
        "                      -1\n",
        "                  )\n",
        "\n",
        "                  cv2.putText(\n",
        "                      annotated,\n",
        "                      current_shot_text,\n",
        "                      (50, y_offset + text_size[1]),\n",
        "                      cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                      1.5,\n",
        "                      (0, 255, 255),  # Ğ–Ñ‘Ğ»Ñ‚Ñ‹Ğ¹ Ñ†Ğ²ĞµÑ‚\n",
        "                      3,\n",
        "                      cv2.LINE_AA\n",
        "                  )\n",
        "\n",
        "                  y_offset += text_size[1] + 15\n",
        "\n",
        "                  if current_shot_distance is not None:\n",
        "                      dist_text = f\"Distance: {current_shot_distance:.1f} foot\"\n",
        "                      cv2.putText(\n",
        "                          annotated,\n",
        "                          dist_text,\n",
        "                          (50, y_offset + 25),\n",
        "                          cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                          0.8,\n",
        "                          (255, 255, 255),  # Ğ‘ĞµĞ»Ñ‹Ğ¹ Ñ†Ğ²ĞµÑ‚\n",
        "                          2,\n",
        "                          cv2.LINE_AA\n",
        "                      )\n",
        "                      y_offset += 40\n",
        "\n",
        "                  if current_shot_result is not None:\n",
        "                      result_text = \"MADE\" if current_shot_result else \"MISSED\"\n",
        "                      color = (0, 255, 0) if current_shot_result else (0, 0, 255)\n",
        "\n",
        "                      cv2.putText(\n",
        "                          annotated,\n",
        "                          result_text,\n",
        "                          (50, y_offset + 25),\n",
        "                          cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                          0.8,\n",
        "                          color,\n",
        "                          2,\n",
        "                          cv2.LINE_AA\n",
        "                      )\n",
        "\n",
        "              # Add slow-motion indicator\n",
        "              annotated = add_slow_motion_indicator(\n",
        "                  frame=annotated,\n",
        "                  slow_motion_active=animation_state.is_animating,\n",
        "                  playback_speed=animation_state.playback_speed\n",
        "              )\n",
        "\n",
        "\n",
        "              # Draw shot markers (existing code)\n",
        "              if have_enough_points and len(shots) > 0:\n",
        "                  made_shots = [s for s in shots if s.result is True]\n",
        "                  missed_shots = [s for s in shots if s.result is False]\n",
        "\n",
        "                  if len(made_shots) > 0:\n",
        "                      made_xy_court = extract_xy(shots=made_shots)\n",
        "                      made_xy_image = court_to_image.transform_points(\n",
        "                          points=made_xy_court\n",
        "                      )\n",
        "                      boxes_xyxy_made = sv.pad_boxes(\n",
        "                          np.hstack((made_xy_image, made_xy_image)),\n",
        "                          px=1, py=1\n",
        "                      )\n",
        "                      classes_made = extract_class_id(shots=made_shots)\n",
        "                      detections_made = sv.Detections(\n",
        "                          xyxy=boxes_xyxy_made, class_id=classes_made\n",
        "                      )\n",
        "                      labels_made = [\n",
        "                          f\"{shot.shot_type}: {int(shot.distance)} feet\"\n",
        "                          for shot in made_shots\n",
        "                      ]\n",
        "                      annotated = triangle_annotator.annotate(\n",
        "                          scene=annotated, detections=detections_made\n",
        "                      )\n",
        "                      annotated = text_annotator.annotate(\n",
        "                          scene=annotated,\n",
        "                          detections=detections_made,\n",
        "                          labels=labels_made,\n",
        "                      )\n",
        "\n",
        "                  if len(missed_shots) > 0:\n",
        "                      missed_xy_court = extract_xy(shots=missed_shots)\n",
        "                      missed_xy_image = court_to_image.transform_points(\n",
        "                          points=missed_xy_court\n",
        "                      )\n",
        "                      boxes_xyxy_missed = sv.pad_boxes(\n",
        "                          np.hstack((missed_xy_image, missed_xy_image)),\n",
        "                          px=1, py=1\n",
        "                      )\n",
        "                      classes_missed = extract_class_id(shots=missed_shots)\n",
        "                      detections_missed = sv.Detections(\n",
        "                          xyxy=boxes_xyxy_missed, class_id=classes_missed\n",
        "                      )\n",
        "                      labels_missed = [\n",
        "                          f\"{shot.shot_type}: missed\"\n",
        "                          for shot in missed_shots\n",
        "                      ]\n",
        "                      annotated = triangle_annotator_missed.annotate(\n",
        "                          scene=annotated, detections=detections_missed\n",
        "                      )\n",
        "                      annotated = text_annotator_missed.annotate(\n",
        "                          scene=annotated,\n",
        "                          detections=detections_missed,\n",
        "                          labels=missed_shots,\n",
        "                      )\n",
        "\n",
        "              sink.write_frame(annotated)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}